{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pytest Error Fixing Framework Welcome to the official documentation for the Pytest Error Fixing Framework. This documentation will help you understand the concepts, usage, and development practices of the framework.","title":"Home"},{"location":"#pytest-error-fixing-framework","text":"Welcome to the official documentation for the Pytest Error Fixing Framework. This documentation will help you understand the concepts, usage, and development practices of the framework.","title":"Pytest Error Fixing Framework"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/","text":"Domain-Driven Design Concepts Guide for pytest-fixer Introduction This guide explains the Domain-Driven Design (DDD) concepts necessary to rebuild pytest-fixer . Each concept is illustrated with concrete examples from our domain. Table of Contents Core DDD Concepts 1. Ubiquitous Language 2. Bounded Contexts 3. Aggregates 4. Entities 5. Value Objects 6. Domain Services 7. Repositories 8. Domain Events 9. Application Services Common DDD Patterns 1. Factory Pattern 2. Specification Pattern 3. Anti-Corruption Layer DDD Best Practices Avoiding Common Mistakes Practical Tips for pytest-fixer Core DDD Concepts 1. Ubiquitous Language The shared language between developers and domain experts. For pytest-fixer , this includes: Test Error : A failing pytest test that needs fixing Fix Attempt : A single try at fixing a test error Fix Generation : The process of creating a fix Verification : Checking if a fix works Code Changes : Modifications made to fix an error Why it matters : Using consistent terminology prevents confusion and misunderstandings. For example, we always say \"fix attempt\" rather than \"try\" or \"fix iteration\". 2. Bounded Contexts Separate domains with their own models and rules. In pytest-fixer : Error Analysis Context Handles test error parsing and analysis Own concept of what an error means Focuses on error details and classification Fix Generation Context Handles creating and applying fixes Manages AI interaction Tracks fix attempts and results Test Execution Context Handles running tests Manages test discovery Processes test results Version Control Context Manages code changes Handles branching strategy Controls commit operations Each context has its own: - Models and rules - Interfaces and services - Data structures and validation 3. Aggregates Clusters of related objects treated as a single unit. Key aggregates in pytest-fixer : 1. TestError Aggregate class TestError: # Aggregate Root id: UUID test_file: Path test_function: str error_details: ErrorDetails # Value Object location: CodeLocation # Value Object fix_attempts: List[FixAttempt] # Child Entity status: FixStatus # Value Object def start_fix_attempt(self, temperature: float) -> FixAttempt: \"\"\"Create and track a new fix attempt\"\"\" 2. FixSession Aggregate class FixSession: # Aggregate Root id: UUID error: TestError current_attempt: Optional[FixAttempt] attempts: List[FixAttempt] status: FixSessionStatus Rules for Aggregates : - Only reference other aggregates by ID - Maintain consistency boundaries - Handle transactional requirements 4. Entities Objects with identity that changes over time. Key entities: 1. FixAttempt @dataclass class FixAttempt: id: UUID error_id: UUID attempt_number: int temperature: float changes: Optional[CodeChanges] status: FixStatus 2. TestCase @dataclass class TestCase: id: UUID file_path: Path function_name: str source_code: str Entity characteristics : - Have unique identity - Mutable over time - Track state changes - Maintain history 5. Value Objects Immutable objects without identity. Examples: @dataclass(frozen=True) class CodeLocation: file_path: Path line_number: int column: Optional[int] = None function_name: Optional[str] = None @dataclass(frozen=True) class ErrorDetails: error_type: str message: str stack_trace: Optional[str] = None captured_output: Optional[str] = None @dataclass(frozen=True) class CodeChanges: original: str modified: str location: CodeLocation description: Optional[str] = None Value Object rules : - Immutable - No identity - Equality based on attributes - Self-validating 6. Domain Services Services that handle operations not belonging to any entity: class ErrorAnalysisService: \"\"\"Analyzes test output to create TestError instances\"\"\" def analyze_error(self, test_output: str, test_file: Path) -> TestError: \"\"\"Extract error information from test output\"\"\" class FixGenerationService: \"\"\"Generates fixes using AI\"\"\" def generate_fix(self, error: TestError, attempt: FixAttempt) -> CodeChanges: \"\"\"Generate a fix for the error\"\"\" When to use Services : - Operation spans multiple entities - Complex domain logic - External system integration 7. Repositories Interfaces for persisting and retrieving aggregates: class TestErrorRepository(Protocol): def get_by_id(self, error_id: UUID) -> Optional[TestError]: \"\"\"Retrieve a TestError by ID\"\"\" def save(self, error: TestError) -> None: \"\"\"Save a TestError\"\"\" def get_unfixed_errors(self) -> List[TestError]: \"\"\"Get all unfixed errors\"\"\" Repository principles : - One repository per aggregate - Hide storage details - Return fully-loaded aggregates - Handle persistence concerns 8. Domain Events Notifications of significant changes in the domain: @dataclass class FixAttemptStarted: error_id: UUID attempt_id: UUID timestamp: datetime @dataclass class FixVerificationCompleted: error_id: UUID attempt_id: UUID success: bool verification_output: str When to use Events : - State changes matter to other contexts - Need to maintain audit trail - Cross-context communication needed 9. Application Services Orchestrate the use cases of the application: class TestFixingApplicationService: def __init__( self, error_analysis: ErrorAnalysisService, fix_generation: FixGenerationService, version_control: VersionControlService, error_repository: TestErrorRepository, event_publisher: EventPublisher ): # Initialize dependencies... def attempt_fix(self, error_id: UUID, temperature: float = 0.4) -> FixAttempt: \"\"\"Coordinate the process of attempting a fix\"\"\" Application Service responsibilities : - Use case orchestration - Transaction management - Event publishing - Error handling Common DDD Patterns 1. Factory Pattern Use factories to create complex aggregates: class TestErrorFactory: def from_test_output( self, test_output: str, test_file: Path, test_function: str ) -> TestError: \"\"\"Create TestError from test output\"\"\" 2. Specification Pattern Express complex queries or validations: class FixableErrorSpecification: def is_satisfied_by(self, error: TestError) -> bool: \"\"\"Check if error can be fixed\"\"\" 3. Anti-Corruption Layer Protect domain model from external systems: class AIServiceAdapter: \"\"\"Adapt AI service responses to our domain model\"\"\" def adapt_response(self, ai_response: dict) -> CodeChanges: \"\"\"Convert AI response to domain model\"\"\" DDD Best Practices Start with Bounded Contexts Identify clear boundaries first Define context interactions Document context maps Focus on Behavior Model behavior, not just data Use rich domain models Encapsulate business rules Use Value Objects Create immutable value objects Validate on creation Make invalid states unrepresentable Handle Edge Cases Define error scenarios Use domain events Maintain consistency Test Domain Logic Unit test aggregates Test business rules Mock infrastructure Avoiding Common Mistakes Anemic Domain Model Don't create data-only classes Include business logic Use rich behavior Leaky Abstractions Keep infrastructure out of domain Use clean interfaces Maintain boundaries Missing Events Use events for important changes Track state transitions Maintain audit trail Complex Aggregates Keep aggregates focused Use proper boundaries Split if too complex Practical Tips for pytest-fixer Start with Core Domain Model (TestError) Add Behavior Incrementally Use Events for Tracking Keep Interfaces Clean Test Domain Logic Thoroughly Additional Blueprint: Aligning on Goals \ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Below is a cohesive, from-scratch rewrite that combines the strengths of previous approaches into a cleaner, domain-driven, and modular architecture. It clarifies domain logic, separates concerns, and provides a strong foundation for future extensions. This blueprint focuses on core functionality: discovering test errors, generating fixes using AI (via a hypothetical AIManager or Coder ), applying changes, verifying them, and persisting state. It employs DDD patterns, a clear layering approach, and sets up a workable starting point. Key Design Principles Domain-Driven Design (DDD) Domain Model : Defines TestError , FixAttempt , ErrorDetails , and related entities as the heart of the domain. Value Objects : CodeLocation , CodeChanges are immutable and model specific domain concepts clearly. Repositories : Abstract away persistence details behind interfaces. Domain Services : Provide business logic that doesn't belong inside entities. Clean Architecture Layers Domain (Core) : Entities, Value Objects, Domain Services, Repository Interfaces. Application : Orchestrates use cases, coordinates domain objects, and triggers domain services. Infrastructure : Implementation details like Git-based repository, AI integration, running pytest , file I/O. Events & Extensibility Define domain events minimally for the starting point. Events can be published to other interested parties (e.g., logging, analytics, asynchronous pipelines). Testing & Configuration Testing can be added incrementally. Configuration handled through environment variables or a config file. Placeholders for integration points ( AIManager , TestRunner , VCSManager ) to be implemented concretely later. Project Structure pytest_fixer/ \u251c\u2500\u2500 domain/ \u2502 \u251c\u2500\u2500 models.py # Entities, Value Objects \u2502 \u251c\u2500\u2500 events.py # Domain events \u2502 \u251c\u2500\u2500 repositories.py # Repository interfaces \u2502 \u251c\u2500\u2500 services.py # Domain services (e.g., ErrorAnalysis) \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 application/ \u2502 \u251c\u2500\u2500 usecases.py # Application services (Use cases) \u2502 \u251c\u2500\u2500 dto.py # Data Transfer Objects if needed \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 ai_manager.py # AI integration (fix generation) \u2502 \u251c\u2500\u2500 test_runner.py # Pytest integration \u2502 \u251c\u2500\u2500 vcs_manager.py # Git operations \u2502 \u251c\u2500\u2500 repository_impl.py# Git or file-based repository implementation \u2502 \u251c\u2500\u2500 change_applier.py # Applying and reverting code changes \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 main.py Detailed Implementation domain/models.py from dataclasses import dataclass, field from datetime import datetime from pathlib import Path from typing import List, Optional from uuid import UUID, uuid4 @dataclass(frozen=True) class CodeLocation: file_path: Path line_number: int column: Optional[int] = None function_name: Optional[str] = None @dataclass(frozen=True) class ErrorDetails: error_type: str message: str stack_trace: Optional[str] = None captured_output: Optional[str] = None @dataclass(frozen=True) class CodeChanges: original: str modified: str description: Optional[str] = None @dataclass class FixAttempt: id: UUID = field(default_factory=uuid4) error_id: UUID = field(default_factory=uuid4) attempt_number: int = 1 temperature: float = 0.4 changes: Optional[CodeChanges] = None status: str = \"pending\" started_at: datetime = field(default_factory=datetime.utcnow) completed_at: Optional[datetime] = None def mark_success(self, changes: CodeChanges): self.changes = changes self.status = \"success\" self.completed_at = datetime.utcnow() def mark_failure(self): self.status = \"failed\" self.completed_at = datetime.utcnow() @dataclass class TestError: id: UUID = field(default_factory=uuid4) test_file: Path = field(default_factory=Path) test_function: str = \"\" error_details: ErrorDetails = field(default_factory=ErrorDetails) location: CodeLocation = field(default_factory=lambda: CodeLocation(Path(\".\"), 0)) fix_attempts: List[FixAttempt] = field(default_factory=list) status: str = \"unfixed\" def start_fix_attempt(self, temperature: float) -> FixAttempt: attempt = FixAttempt( error_id=self.id, attempt_number=len(self.fix_attempts) + 1, temperature=temperature ) self.fix_attempts.append(attempt) return attempt def mark_fixed(self, attempt: FixAttempt): attempt.status = \"success\" self.status = \"fixed\" def mark_attempt_failed(self, attempt: FixAttempt): attempt.mark_failure() def can_attempt_fix(self) -> bool: return self.status == \"unfixed\" domain/events.py from dataclasses import dataclass from datetime import datetime from uuid import UUID @dataclass class FixAttemptStarted: error_id: UUID attempt_id: UUID timestamp: datetime @dataclass class FixAttemptCompleted: error_id: UUID attempt_id: UUID success: bool timestamp: datetime domain/repositories.py from abc import ABC, abstractmethod from typing import Optional, List from uuid import UUID from .models import TestError class TestErrorRepository(ABC): @abstractmethod def get_by_id(self, error_id: UUID) -> Optional[TestError]: pass @abstractmethod def save(self, error: TestError) -> None: pass @abstractmethod def get_unfixed_errors(self) -> List[TestError]: pass domain/services.py import re from pathlib import Path from typing import Optional, List from .models import ErrorDetails, TestError, CodeLocation class ErrorAnalysisService: def analyze_errors(self, test_output: str) -> Optional[List[TestError]]: # Basic regex-based approach to find failing tests: # Placeholder: Real logic might integrate directly with pytest APIs. pattern = r\"(.*?::(.*?) FAILED (.*)\\n([\\s\\S]*?)(?=\\n\\n|$))\" matches = re.finditer(pattern, test_output) errors = [] for m in matches: file_path, test_func, err_type, details = m.groups() location = CodeLocation(Path(file_path), 0) err_details = ErrorDetails( error_type=err_type.strip(), message=details.strip(), stack_trace=details ) errors.append(TestError( test_file=Path(file_path), test_function=test_func, error_details=err_details, location=location )) return errors if errors else None application/usecases.py from typing import Optional from uuid import UUID from ..domain.models import TestError from ..domain.repositories import TestErrorRepository from ..infrastructure.ai_manager import AIManager from ..infrastructure.test_runner import TestRunner from ..infrastructure.vcs_manager import VCSManager from ..infrastructure.change_applier import ChangeApplier class TestFixingService: def __init__( self, error_repo: TestErrorRepository, ai_manager: AIManager, test_runner: TestRunner, vcs_manager: VCSManager, change_applier: ChangeApplier, initial_temp: float = 0.4, temp_increment: float = 0.1, max_retries: int = 3 ): self.error_repo = error_repo self.ai_manager = ai_manager self.test_runner = test_runner self.vcs = vcs_manager self.change_applier = change_applier self.initial_temp = initial_temp self.temp_increment = temp_increment self.max_retries = max_retries def discover_and_record_errors(self) -> None: stdout, stderr = self.test_runner.run_all_tests() analysis_service = self.test_runner.get_analysis_service() errors = analysis_service.analyze_errors(stdout + stderr) if not errors: return for e in errors: self.error_repo.save(e) def attempt_fix(self, error_id: UUID) -> bool: error = self.error_repo.get_by_id(error_id) if not error or not error.can_attempt_fix(): return False temperature = self.initial_temp for _ in range(self.max_retries): attempt = error.start_fix_attempt(temperature) # Generate fix changes = self.ai_manager.generate_fix(error, temperature) if not changes: # No fix generated, increase temp and continue error.mark_attempt_failed(attempt) temperature += self.temp_increment continue # Apply changes self.change_applier.apply(changes, error.location) # Verify fix passed = self.test_runner.run_test_and_check( error.test_file, error.test_function ) if passed: # Commit changes to VCS self.vcs.commit_changes(f\"Fix {error.test_file}:{error.test_function}\") attempt.mark_success(changes) error.mark_fixed(attempt) self.error_repo.save(error) return True else: # Revert changes and mark attempt failed self.change_applier.revert() error.mark_attempt_failed(attempt) self.error_repo.save(error) temperature += self.temp_increment # No success after max retries self.error_repo.save(error) return False infrastructure/ai_manager.py from typing import Optional from ..domain.models import TestError, CodeChanges class AIManager: def __init__(self, model_name: str = \"gpt-4\"): self.model_name = model_name def generate_fix(self, error: TestError, temperature: float) -> Optional[CodeChanges]: # Placeholder logic - integrate with OpenAI or Aider coder # Construct a prompt from error details prompt = f\"Fix this test error:\\n{error.error_details.message}\" # Here you'd call the AI API or Aider coder to get a changed version of the file # For demonstration, let's pretend we got a changed file content: modified = error.location.file_path.read_text().replace(\"bug\", \"fix\") original = error.location.file_path.read_text() return CodeChanges(original=original, modified=modified, description=\"AI suggested fix\") infrastructure/test_runner.py from typing import Tuple from .ai_manager import AIManager from ..domain.services import ErrorAnalysisService from pathlib import Path import subprocess class TestRunner: def __init__(self, project_dir: Path): self.project_dir = project_dir self.analysis_service = ErrorAnalysisService() def run_all_tests(self) -> Tuple[str, str]: cmd = [\"pytest\", \"-v\"] result = subprocess.run(cmd, cwd=self.project_dir, capture_output=True, text=True) return result.stdout, result.stderr def run_test_and_check(self, test_file: Path, test_function: str) -> bool: cmd = [\"pytest\", \"-q\", f\"{test_file}::{test_function}\"] result = subprocess.run(cmd, cwd=self.project_dir, capture_output=True, text=True) # If it returns 0 and '1 passed' is in stdout, we consider it passed return (result.returncode == 0 and \"1 passed\" in result.stdout.lower()) def get_analysis_service(self) -> ErrorAnalysisService: return self.analysis_service infrastructure/vcs_manager.py import subprocess from pathlib import Path class VCSManager: def __init__(self, project_dir: Path): self.project_dir = project_dir def commit_changes(self, message: str): subprocess.run([\"git\", \"add\", \".\"], cwd=self.project_dir, check=True) subprocess.run([\"git\", \"commit\", \"-m\", message], cwd=self.project_dir, check=True) infrastructure/change_applier.py from ..domain.models import CodeChanges, CodeLocation class ChangeApplier: def __init__(self): self.backup_content = None self.target_file = None def apply(self, changes: CodeChanges, location: CodeLocation): self.target_file = location.file_path self.backup_content = self.target_file.read_text() self.target_file.write_text(changes.modified) def revert(self): if self.target_file and self.backup_content is not None: self.target_file.write_text(self.backup_content) infrastructure/repository_impl.py from uuid import UUID from typing import Optional, List from pathlib import Path import json from ..domain.models import TestError from ..domain.repositories import TestErrorRepository class FileBasedTestErrorRepository(TestErrorRepository): \"\"\"Simple file-based repository using JSON to store errors.\"\"\" def __init__(self, storage_path: Path): self.storage_path = storage_path self.storage_path.mkdir(parents=True, exist_ok=True) def _get_error_file(self, error_id: UUID) -> Path: return self.storage_path / f\"{error_id}.json\" def get_by_id(self, error_id: UUID) -> Optional[TestError]: f = self._get_error_file(error_id) if not f.exists(): return None data = json.loads(f.read_text()) return self._deserialize_testerror(data) def save(self, error: TestError) -> None: f = self._get_error_file(error.id) f.write_text(json.dumps(self._serialize_testerror(error), indent=2)) def get_unfixed_errors(self) -> List[TestError]: errors = [] for f in self.storage_path.glob(\"*.json\"): data = json.loads(f.read_text()) err = self._deserialize_testerror(data) if err.status == \"unfixed\": errors.append(err) return errors def _serialize_testerror(self, error: TestError) -> dict: return { \"id\": str(error.id), \"test_file\": str(error.test_file), \"test_function\": error.test_function, \"status\": error.status, \"error_details\": { \"error_type\": error.error_details.error_type, \"message\": error.error_details.message, \"stack_trace\": error.error_details.stack_trace, \"captured_output\": error.error_details.captured_output }, \"location\": { \"file_path\": str(error.location.file_path), \"line_number\": error.location.line_number, \"column\": error.location.column }, \"fix_attempts\": [ { \"id\": str(a.id), \"error_id\": str(a.error_id), \"attempt_number\": a.attempt_number, \"temperature\": a.temperature, \"status\": a.status, \"started_at\": a.started_at.isoformat(), \"completed_at\": a.completed_at.isoformat() if a.completed_at else None, \"changes\": { \"original\": a.changes.original if a.changes else None, \"modified\": a.changes.modified if a.changes else None, \"description\": a.changes.description if a.changes else None } if a.changes else None } for a in error.fix_attempts ] } def _deserialize_testerror(self, data: dict) -> TestError: from datetime import datetime from uuid import UUID from ..domain.models import ErrorDetails, CodeLocation, CodeChanges, FixAttempt, TestError fix_attempts = [] for a in data.get(\"fix_attempts\", []): changes = a[\"changes\"] fix_attempts.append(FixAttempt( id=UUID(a[\"id\"]), error_id=UUID(a[\"error_id\"]), attempt_number=a[\"attempt_number\"], temperature=a[\"temperature\"], status=a[\"status\"], started_at=datetime.fromisoformat(a[\"started_at\"]), completed_at=datetime.fromisoformat(a[\"completed_at\"]) if a[\"completed_at\"] else None, changes=CodeChanges(**changes) if changes and changes[\"original\"] else None )) return TestError( id=UUID(data[\"id\"]), test_file=Path(data[\"test_file\"]), test_function=data[\"test_function\"], status=data[\"status\"], error_details=ErrorDetails(**data[\"error_details\"]), location=CodeLocation( Path(data[\"location\"][\"file_path\"]), data[\"location\"][\"line_number\"], data[\"location\"][\"column\"] ), fix_attempts=fix_attempts ) main.py (Example Entry Point) import sys from pathlib import Path from uuid import UUID from pytest_fixer.domain.repositories import TestErrorRepository from pytest_fixer.infrastructure.repository_impl import FileBasedTestErrorRepository from pytest_fixer.infrastructure.ai_manager import AIManager from pytest_fixer.infrastructure.test_runner import TestRunner from pytest_fixer.infrastructure.vcs_manager import VCSManager from pytest_fixer.infrastructure.change_applier import ChangeApplier from pytest_fixer.application.usecases import TestFixingService def main(): project_dir = Path(\".\") storage_path = project_dir / \".pytest_fixer_storage\" error_repo: TestErrorRepository = FileBasedTestErrorRepository(storage_path) ai_manager = AIManager(model_name=\"gpt-4\") test_runner = TestRunner(project_dir) vcs_manager = VCSManager(project_dir) change_applier = ChangeApplier() service = TestFixingService( error_repo=error_repo, ai_manager=ai_manager, test_runner=test_runner, vcs_manager=vcs_manager, change_applier=change_applier, initial_temp=0.4, temp_increment=0.1, max_retries=3 ) # Discover new errors service.discover_and_record_errors() # Attempt fix on all unfixed errors unfixed = error_repo.get_unfixed_errors() for err in unfixed: print(f\"Attempting to fix error {err.id} in {err.test_file}:{err.test_function}\") success = service.attempt_fix(err.id) if success: print(f\"Error {err.id} fixed!\") else: print(f\"Failed to fix error {err.id}\") if __name__ == \"__main__\": main() Next Steps Add Tests : Unit tests for domain models, application services, and repositories. Robust Error Analysis : Improve parsing logic to handle real pytest output. Real AI Integration : Implement AIManager to communicate with OpenAI or a local LLM. Enhanced Verification : Capture test output logs, enable incremental verification, and implement more robust revert strategies. Event Handling : Integrate event dispatchers or log all domain events as needed. Configuration & Logging : Integrate a configuration file or environment variables and a structured logging solution. This setup is cleaner, better modularized, and follows best practices by combining a domain-driven structure with a clear separation of concerns. It enhances maintainability and extensibility, providing a solid foundation that can be refined and expanded as the project\u2019s complexity grows. Visual Enhancements To further enhance the readability and visual appeal of your Markdown document, consider the following tips: Consistent Heading Levels : Ensure that heading levels ( # , ## , ### , etc.) are used consistently to represent the document structure. Code Blocks : Use triple backticks (```) for code blocks with proper syntax highlighting by specifying the language (e.g., ```python). Lists and Indentation : Use bullet points or numbered lists to organize information clearly. Bold and Italics : Highlight key terms and important points using bold or italics . Tables : For comparing options or presenting structured data, use Markdown tables. Spacing : Add blank lines between sections and elements to prevent clutter and improve readability.","title":"DDD for Pytest Fixer Concepts and Implementation Guide"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#domain-driven-design-concepts-guide-for-pytest-fixer","text":"","title":"Domain-Driven Design Concepts Guide for pytest-fixer"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#introduction","text":"This guide explains the Domain-Driven Design (DDD) concepts necessary to rebuild pytest-fixer . Each concept is illustrated with concrete examples from our domain.","title":"Introduction"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#table-of-contents","text":"Core DDD Concepts 1. Ubiquitous Language 2. Bounded Contexts 3. Aggregates 4. Entities 5. Value Objects 6. Domain Services 7. Repositories 8. Domain Events 9. Application Services Common DDD Patterns 1. Factory Pattern 2. Specification Pattern 3. Anti-Corruption Layer DDD Best Practices Avoiding Common Mistakes Practical Tips for pytest-fixer","title":"Table of Contents"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#core-ddd-concepts","text":"","title":"Core DDD Concepts"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#1-ubiquitous-language","text":"The shared language between developers and domain experts. For pytest-fixer , this includes: Test Error : A failing pytest test that needs fixing Fix Attempt : A single try at fixing a test error Fix Generation : The process of creating a fix Verification : Checking if a fix works Code Changes : Modifications made to fix an error Why it matters : Using consistent terminology prevents confusion and misunderstandings. For example, we always say \"fix attempt\" rather than \"try\" or \"fix iteration\".","title":"1. Ubiquitous Language"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#2-bounded-contexts","text":"Separate domains with their own models and rules. In pytest-fixer : Error Analysis Context Handles test error parsing and analysis Own concept of what an error means Focuses on error details and classification Fix Generation Context Handles creating and applying fixes Manages AI interaction Tracks fix attempts and results Test Execution Context Handles running tests Manages test discovery Processes test results Version Control Context Manages code changes Handles branching strategy Controls commit operations Each context has its own: - Models and rules - Interfaces and services - Data structures and validation","title":"2. Bounded Contexts"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#3-aggregates","text":"Clusters of related objects treated as a single unit. Key aggregates in pytest-fixer :","title":"3. Aggregates"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#1-testerror-aggregate","text":"class TestError: # Aggregate Root id: UUID test_file: Path test_function: str error_details: ErrorDetails # Value Object location: CodeLocation # Value Object fix_attempts: List[FixAttempt] # Child Entity status: FixStatus # Value Object def start_fix_attempt(self, temperature: float) -> FixAttempt: \"\"\"Create and track a new fix attempt\"\"\"","title":"1. TestError Aggregate"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#2-fixsession-aggregate","text":"class FixSession: # Aggregate Root id: UUID error: TestError current_attempt: Optional[FixAttempt] attempts: List[FixAttempt] status: FixSessionStatus Rules for Aggregates : - Only reference other aggregates by ID - Maintain consistency boundaries - Handle transactional requirements","title":"2. FixSession Aggregate"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#4-entities","text":"Objects with identity that changes over time. Key entities:","title":"4. Entities"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#1-fixattempt","text":"@dataclass class FixAttempt: id: UUID error_id: UUID attempt_number: int temperature: float changes: Optional[CodeChanges] status: FixStatus","title":"1. FixAttempt"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#2-testcase","text":"@dataclass class TestCase: id: UUID file_path: Path function_name: str source_code: str Entity characteristics : - Have unique identity - Mutable over time - Track state changes - Maintain history","title":"2. TestCase"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#5-value-objects","text":"Immutable objects without identity. Examples: @dataclass(frozen=True) class CodeLocation: file_path: Path line_number: int column: Optional[int] = None function_name: Optional[str] = None @dataclass(frozen=True) class ErrorDetails: error_type: str message: str stack_trace: Optional[str] = None captured_output: Optional[str] = None @dataclass(frozen=True) class CodeChanges: original: str modified: str location: CodeLocation description: Optional[str] = None Value Object rules : - Immutable - No identity - Equality based on attributes - Self-validating","title":"5. Value Objects"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#6-domain-services","text":"Services that handle operations not belonging to any entity: class ErrorAnalysisService: \"\"\"Analyzes test output to create TestError instances\"\"\" def analyze_error(self, test_output: str, test_file: Path) -> TestError: \"\"\"Extract error information from test output\"\"\" class FixGenerationService: \"\"\"Generates fixes using AI\"\"\" def generate_fix(self, error: TestError, attempt: FixAttempt) -> CodeChanges: \"\"\"Generate a fix for the error\"\"\" When to use Services : - Operation spans multiple entities - Complex domain logic - External system integration","title":"6. Domain Services"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#7-repositories","text":"Interfaces for persisting and retrieving aggregates: class TestErrorRepository(Protocol): def get_by_id(self, error_id: UUID) -> Optional[TestError]: \"\"\"Retrieve a TestError by ID\"\"\" def save(self, error: TestError) -> None: \"\"\"Save a TestError\"\"\" def get_unfixed_errors(self) -> List[TestError]: \"\"\"Get all unfixed errors\"\"\" Repository principles : - One repository per aggregate - Hide storage details - Return fully-loaded aggregates - Handle persistence concerns","title":"7. Repositories"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#8-domain-events","text":"Notifications of significant changes in the domain: @dataclass class FixAttemptStarted: error_id: UUID attempt_id: UUID timestamp: datetime @dataclass class FixVerificationCompleted: error_id: UUID attempt_id: UUID success: bool verification_output: str When to use Events : - State changes matter to other contexts - Need to maintain audit trail - Cross-context communication needed","title":"8. Domain Events"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#9-application-services","text":"Orchestrate the use cases of the application: class TestFixingApplicationService: def __init__( self, error_analysis: ErrorAnalysisService, fix_generation: FixGenerationService, version_control: VersionControlService, error_repository: TestErrorRepository, event_publisher: EventPublisher ): # Initialize dependencies... def attempt_fix(self, error_id: UUID, temperature: float = 0.4) -> FixAttempt: \"\"\"Coordinate the process of attempting a fix\"\"\" Application Service responsibilities : - Use case orchestration - Transaction management - Event publishing - Error handling","title":"9. Application Services"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#common-ddd-patterns","text":"","title":"Common DDD Patterns"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#1-factory-pattern","text":"Use factories to create complex aggregates: class TestErrorFactory: def from_test_output( self, test_output: str, test_file: Path, test_function: str ) -> TestError: \"\"\"Create TestError from test output\"\"\"","title":"1. Factory Pattern"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#2-specification-pattern","text":"Express complex queries or validations: class FixableErrorSpecification: def is_satisfied_by(self, error: TestError) -> bool: \"\"\"Check if error can be fixed\"\"\"","title":"2. Specification Pattern"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#3-anti-corruption-layer","text":"Protect domain model from external systems: class AIServiceAdapter: \"\"\"Adapt AI service responses to our domain model\"\"\" def adapt_response(self, ai_response: dict) -> CodeChanges: \"\"\"Convert AI response to domain model\"\"\"","title":"3. Anti-Corruption Layer"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#ddd-best-practices","text":"Start with Bounded Contexts Identify clear boundaries first Define context interactions Document context maps Focus on Behavior Model behavior, not just data Use rich domain models Encapsulate business rules Use Value Objects Create immutable value objects Validate on creation Make invalid states unrepresentable Handle Edge Cases Define error scenarios Use domain events Maintain consistency Test Domain Logic Unit test aggregates Test business rules Mock infrastructure","title":"DDD Best Practices"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#avoiding-common-mistakes","text":"Anemic Domain Model Don't create data-only classes Include business logic Use rich behavior Leaky Abstractions Keep infrastructure out of domain Use clean interfaces Maintain boundaries Missing Events Use events for important changes Track state transitions Maintain audit trail Complex Aggregates Keep aggregates focused Use proper boundaries Split if too complex","title":"Avoiding Common Mistakes"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#practical-tips-for-pytest-fixer","text":"Start with Core Domain Model (TestError) Add Behavior Incrementally Use Events for Tracking Keep Interfaces Clean Test Domain Logic Thoroughly","title":"Practical Tips for pytest-fixer"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#additional-blueprint-aligning-on-goals","text":"\ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f: Below is a cohesive, from-scratch rewrite that combines the strengths of previous approaches into a cleaner, domain-driven, and modular architecture. It clarifies domain logic, separates concerns, and provides a strong foundation for future extensions. This blueprint focuses on core functionality: discovering test errors, generating fixes using AI (via a hypothetical AIManager or Coder ), applying changes, verifying them, and persisting state. It employs DDD patterns, a clear layering approach, and sets up a workable starting point.","title":"Additional Blueprint: Aligning on Goals"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#key-design-principles","text":"Domain-Driven Design (DDD) Domain Model : Defines TestError , FixAttempt , ErrorDetails , and related entities as the heart of the domain. Value Objects : CodeLocation , CodeChanges are immutable and model specific domain concepts clearly. Repositories : Abstract away persistence details behind interfaces. Domain Services : Provide business logic that doesn't belong inside entities. Clean Architecture Layers Domain (Core) : Entities, Value Objects, Domain Services, Repository Interfaces. Application : Orchestrates use cases, coordinates domain objects, and triggers domain services. Infrastructure : Implementation details like Git-based repository, AI integration, running pytest , file I/O. Events & Extensibility Define domain events minimally for the starting point. Events can be published to other interested parties (e.g., logging, analytics, asynchronous pipelines). Testing & Configuration Testing can be added incrementally. Configuration handled through environment variables or a config file. Placeholders for integration points ( AIManager , TestRunner , VCSManager ) to be implemented concretely later.","title":"Key Design Principles"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#project-structure","text":"pytest_fixer/ \u251c\u2500\u2500 domain/ \u2502 \u251c\u2500\u2500 models.py # Entities, Value Objects \u2502 \u251c\u2500\u2500 events.py # Domain events \u2502 \u251c\u2500\u2500 repositories.py # Repository interfaces \u2502 \u251c\u2500\u2500 services.py # Domain services (e.g., ErrorAnalysis) \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 application/ \u2502 \u251c\u2500\u2500 usecases.py # Application services (Use cases) \u2502 \u251c\u2500\u2500 dto.py # Data Transfer Objects if needed \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 ai_manager.py # AI integration (fix generation) \u2502 \u251c\u2500\u2500 test_runner.py # Pytest integration \u2502 \u251c\u2500\u2500 vcs_manager.py # Git operations \u2502 \u251c\u2500\u2500 repository_impl.py# Git or file-based repository implementation \u2502 \u251c\u2500\u2500 change_applier.py # Applying and reverting code changes \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 main.py","title":"Project Structure"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#detailed-implementation","text":"","title":"Detailed Implementation"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#domainmodelspy","text":"from dataclasses import dataclass, field from datetime import datetime from pathlib import Path from typing import List, Optional from uuid import UUID, uuid4 @dataclass(frozen=True) class CodeLocation: file_path: Path line_number: int column: Optional[int] = None function_name: Optional[str] = None @dataclass(frozen=True) class ErrorDetails: error_type: str message: str stack_trace: Optional[str] = None captured_output: Optional[str] = None @dataclass(frozen=True) class CodeChanges: original: str modified: str description: Optional[str] = None @dataclass class FixAttempt: id: UUID = field(default_factory=uuid4) error_id: UUID = field(default_factory=uuid4) attempt_number: int = 1 temperature: float = 0.4 changes: Optional[CodeChanges] = None status: str = \"pending\" started_at: datetime = field(default_factory=datetime.utcnow) completed_at: Optional[datetime] = None def mark_success(self, changes: CodeChanges): self.changes = changes self.status = \"success\" self.completed_at = datetime.utcnow() def mark_failure(self): self.status = \"failed\" self.completed_at = datetime.utcnow() @dataclass class TestError: id: UUID = field(default_factory=uuid4) test_file: Path = field(default_factory=Path) test_function: str = \"\" error_details: ErrorDetails = field(default_factory=ErrorDetails) location: CodeLocation = field(default_factory=lambda: CodeLocation(Path(\".\"), 0)) fix_attempts: List[FixAttempt] = field(default_factory=list) status: str = \"unfixed\" def start_fix_attempt(self, temperature: float) -> FixAttempt: attempt = FixAttempt( error_id=self.id, attempt_number=len(self.fix_attempts) + 1, temperature=temperature ) self.fix_attempts.append(attempt) return attempt def mark_fixed(self, attempt: FixAttempt): attempt.status = \"success\" self.status = \"fixed\" def mark_attempt_failed(self, attempt: FixAttempt): attempt.mark_failure() def can_attempt_fix(self) -> bool: return self.status == \"unfixed\"","title":"domain/models.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#domaineventspy","text":"from dataclasses import dataclass from datetime import datetime from uuid import UUID @dataclass class FixAttemptStarted: error_id: UUID attempt_id: UUID timestamp: datetime @dataclass class FixAttemptCompleted: error_id: UUID attempt_id: UUID success: bool timestamp: datetime","title":"domain/events.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#domainrepositoriespy","text":"from abc import ABC, abstractmethod from typing import Optional, List from uuid import UUID from .models import TestError class TestErrorRepository(ABC): @abstractmethod def get_by_id(self, error_id: UUID) -> Optional[TestError]: pass @abstractmethod def save(self, error: TestError) -> None: pass @abstractmethod def get_unfixed_errors(self) -> List[TestError]: pass","title":"domain/repositories.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#domainservicespy","text":"import re from pathlib import Path from typing import Optional, List from .models import ErrorDetails, TestError, CodeLocation class ErrorAnalysisService: def analyze_errors(self, test_output: str) -> Optional[List[TestError]]: # Basic regex-based approach to find failing tests: # Placeholder: Real logic might integrate directly with pytest APIs. pattern = r\"(.*?::(.*?) FAILED (.*)\\n([\\s\\S]*?)(?=\\n\\n|$))\" matches = re.finditer(pattern, test_output) errors = [] for m in matches: file_path, test_func, err_type, details = m.groups() location = CodeLocation(Path(file_path), 0) err_details = ErrorDetails( error_type=err_type.strip(), message=details.strip(), stack_trace=details ) errors.append(TestError( test_file=Path(file_path), test_function=test_func, error_details=err_details, location=location )) return errors if errors else None","title":"domain/services.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#applicationusecasespy","text":"from typing import Optional from uuid import UUID from ..domain.models import TestError from ..domain.repositories import TestErrorRepository from ..infrastructure.ai_manager import AIManager from ..infrastructure.test_runner import TestRunner from ..infrastructure.vcs_manager import VCSManager from ..infrastructure.change_applier import ChangeApplier class TestFixingService: def __init__( self, error_repo: TestErrorRepository, ai_manager: AIManager, test_runner: TestRunner, vcs_manager: VCSManager, change_applier: ChangeApplier, initial_temp: float = 0.4, temp_increment: float = 0.1, max_retries: int = 3 ): self.error_repo = error_repo self.ai_manager = ai_manager self.test_runner = test_runner self.vcs = vcs_manager self.change_applier = change_applier self.initial_temp = initial_temp self.temp_increment = temp_increment self.max_retries = max_retries def discover_and_record_errors(self) -> None: stdout, stderr = self.test_runner.run_all_tests() analysis_service = self.test_runner.get_analysis_service() errors = analysis_service.analyze_errors(stdout + stderr) if not errors: return for e in errors: self.error_repo.save(e) def attempt_fix(self, error_id: UUID) -> bool: error = self.error_repo.get_by_id(error_id) if not error or not error.can_attempt_fix(): return False temperature = self.initial_temp for _ in range(self.max_retries): attempt = error.start_fix_attempt(temperature) # Generate fix changes = self.ai_manager.generate_fix(error, temperature) if not changes: # No fix generated, increase temp and continue error.mark_attempt_failed(attempt) temperature += self.temp_increment continue # Apply changes self.change_applier.apply(changes, error.location) # Verify fix passed = self.test_runner.run_test_and_check( error.test_file, error.test_function ) if passed: # Commit changes to VCS self.vcs.commit_changes(f\"Fix {error.test_file}:{error.test_function}\") attempt.mark_success(changes) error.mark_fixed(attempt) self.error_repo.save(error) return True else: # Revert changes and mark attempt failed self.change_applier.revert() error.mark_attempt_failed(attempt) self.error_repo.save(error) temperature += self.temp_increment # No success after max retries self.error_repo.save(error) return False","title":"application/usecases.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#infrastructureai_managerpy","text":"from typing import Optional from ..domain.models import TestError, CodeChanges class AIManager: def __init__(self, model_name: str = \"gpt-4\"): self.model_name = model_name def generate_fix(self, error: TestError, temperature: float) -> Optional[CodeChanges]: # Placeholder logic - integrate with OpenAI or Aider coder # Construct a prompt from error details prompt = f\"Fix this test error:\\n{error.error_details.message}\" # Here you'd call the AI API or Aider coder to get a changed version of the file # For demonstration, let's pretend we got a changed file content: modified = error.location.file_path.read_text().replace(\"bug\", \"fix\") original = error.location.file_path.read_text() return CodeChanges(original=original, modified=modified, description=\"AI suggested fix\")","title":"infrastructure/ai_manager.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#infrastructuretest_runnerpy","text":"from typing import Tuple from .ai_manager import AIManager from ..domain.services import ErrorAnalysisService from pathlib import Path import subprocess class TestRunner: def __init__(self, project_dir: Path): self.project_dir = project_dir self.analysis_service = ErrorAnalysisService() def run_all_tests(self) -> Tuple[str, str]: cmd = [\"pytest\", \"-v\"] result = subprocess.run(cmd, cwd=self.project_dir, capture_output=True, text=True) return result.stdout, result.stderr def run_test_and_check(self, test_file: Path, test_function: str) -> bool: cmd = [\"pytest\", \"-q\", f\"{test_file}::{test_function}\"] result = subprocess.run(cmd, cwd=self.project_dir, capture_output=True, text=True) # If it returns 0 and '1 passed' is in stdout, we consider it passed return (result.returncode == 0 and \"1 passed\" in result.stdout.lower()) def get_analysis_service(self) -> ErrorAnalysisService: return self.analysis_service","title":"infrastructure/test_runner.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#infrastructurevcs_managerpy","text":"import subprocess from pathlib import Path class VCSManager: def __init__(self, project_dir: Path): self.project_dir = project_dir def commit_changes(self, message: str): subprocess.run([\"git\", \"add\", \".\"], cwd=self.project_dir, check=True) subprocess.run([\"git\", \"commit\", \"-m\", message], cwd=self.project_dir, check=True)","title":"infrastructure/vcs_manager.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#infrastructurechange_applierpy","text":"from ..domain.models import CodeChanges, CodeLocation class ChangeApplier: def __init__(self): self.backup_content = None self.target_file = None def apply(self, changes: CodeChanges, location: CodeLocation): self.target_file = location.file_path self.backup_content = self.target_file.read_text() self.target_file.write_text(changes.modified) def revert(self): if self.target_file and self.backup_content is not None: self.target_file.write_text(self.backup_content)","title":"infrastructure/change_applier.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#infrastructurerepository_implpy","text":"from uuid import UUID from typing import Optional, List from pathlib import Path import json from ..domain.models import TestError from ..domain.repositories import TestErrorRepository class FileBasedTestErrorRepository(TestErrorRepository): \"\"\"Simple file-based repository using JSON to store errors.\"\"\" def __init__(self, storage_path: Path): self.storage_path = storage_path self.storage_path.mkdir(parents=True, exist_ok=True) def _get_error_file(self, error_id: UUID) -> Path: return self.storage_path / f\"{error_id}.json\" def get_by_id(self, error_id: UUID) -> Optional[TestError]: f = self._get_error_file(error_id) if not f.exists(): return None data = json.loads(f.read_text()) return self._deserialize_testerror(data) def save(self, error: TestError) -> None: f = self._get_error_file(error.id) f.write_text(json.dumps(self._serialize_testerror(error), indent=2)) def get_unfixed_errors(self) -> List[TestError]: errors = [] for f in self.storage_path.glob(\"*.json\"): data = json.loads(f.read_text()) err = self._deserialize_testerror(data) if err.status == \"unfixed\": errors.append(err) return errors def _serialize_testerror(self, error: TestError) -> dict: return { \"id\": str(error.id), \"test_file\": str(error.test_file), \"test_function\": error.test_function, \"status\": error.status, \"error_details\": { \"error_type\": error.error_details.error_type, \"message\": error.error_details.message, \"stack_trace\": error.error_details.stack_trace, \"captured_output\": error.error_details.captured_output }, \"location\": { \"file_path\": str(error.location.file_path), \"line_number\": error.location.line_number, \"column\": error.location.column }, \"fix_attempts\": [ { \"id\": str(a.id), \"error_id\": str(a.error_id), \"attempt_number\": a.attempt_number, \"temperature\": a.temperature, \"status\": a.status, \"started_at\": a.started_at.isoformat(), \"completed_at\": a.completed_at.isoformat() if a.completed_at else None, \"changes\": { \"original\": a.changes.original if a.changes else None, \"modified\": a.changes.modified if a.changes else None, \"description\": a.changes.description if a.changes else None } if a.changes else None } for a in error.fix_attempts ] } def _deserialize_testerror(self, data: dict) -> TestError: from datetime import datetime from uuid import UUID from ..domain.models import ErrorDetails, CodeLocation, CodeChanges, FixAttempt, TestError fix_attempts = [] for a in data.get(\"fix_attempts\", []): changes = a[\"changes\"] fix_attempts.append(FixAttempt( id=UUID(a[\"id\"]), error_id=UUID(a[\"error_id\"]), attempt_number=a[\"attempt_number\"], temperature=a[\"temperature\"], status=a[\"status\"], started_at=datetime.fromisoformat(a[\"started_at\"]), completed_at=datetime.fromisoformat(a[\"completed_at\"]) if a[\"completed_at\"] else None, changes=CodeChanges(**changes) if changes and changes[\"original\"] else None )) return TestError( id=UUID(data[\"id\"]), test_file=Path(data[\"test_file\"]), test_function=data[\"test_function\"], status=data[\"status\"], error_details=ErrorDetails(**data[\"error_details\"]), location=CodeLocation( Path(data[\"location\"][\"file_path\"]), data[\"location\"][\"line_number\"], data[\"location\"][\"column\"] ), fix_attempts=fix_attempts )","title":"infrastructure/repository_impl.py"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#mainpy-example-entry-point","text":"import sys from pathlib import Path from uuid import UUID from pytest_fixer.domain.repositories import TestErrorRepository from pytest_fixer.infrastructure.repository_impl import FileBasedTestErrorRepository from pytest_fixer.infrastructure.ai_manager import AIManager from pytest_fixer.infrastructure.test_runner import TestRunner from pytest_fixer.infrastructure.vcs_manager import VCSManager from pytest_fixer.infrastructure.change_applier import ChangeApplier from pytest_fixer.application.usecases import TestFixingService def main(): project_dir = Path(\".\") storage_path = project_dir / \".pytest_fixer_storage\" error_repo: TestErrorRepository = FileBasedTestErrorRepository(storage_path) ai_manager = AIManager(model_name=\"gpt-4\") test_runner = TestRunner(project_dir) vcs_manager = VCSManager(project_dir) change_applier = ChangeApplier() service = TestFixingService( error_repo=error_repo, ai_manager=ai_manager, test_runner=test_runner, vcs_manager=vcs_manager, change_applier=change_applier, initial_temp=0.4, temp_increment=0.1, max_retries=3 ) # Discover new errors service.discover_and_record_errors() # Attempt fix on all unfixed errors unfixed = error_repo.get_unfixed_errors() for err in unfixed: print(f\"Attempting to fix error {err.id} in {err.test_file}:{err.test_function}\") success = service.attempt_fix(err.id) if success: print(f\"Error {err.id} fixed!\") else: print(f\"Failed to fix error {err.id}\") if __name__ == \"__main__\": main()","title":"main.py (Example Entry Point)"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#next-steps","text":"Add Tests : Unit tests for domain models, application services, and repositories. Robust Error Analysis : Improve parsing logic to handle real pytest output. Real AI Integration : Implement AIManager to communicate with OpenAI or a local LLM. Enhanced Verification : Capture test output logs, enable incremental verification, and implement more robust revert strategies. Event Handling : Integrate event dispatchers or log all domain events as needed. Configuration & Logging : Integrate a configuration file or environment variables and a structured logging solution. This setup is cleaner, better modularized, and follows best practices by combining a domain-driven structure with a clear separation of concerns. It enhances maintainability and extensibility, providing a solid foundation that can be refined and expanded as the project\u2019s complexity grows.","title":"Next Steps"},{"location":"DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/#visual-enhancements","text":"To further enhance the readability and visual appeal of your Markdown document, consider the following tips: Consistent Heading Levels : Ensure that heading levels ( # , ## , ### , etc.) are used consistently to represent the document structure. Code Blocks : Use triple backticks (```) for code blocks with proper syntax highlighting by specifying the language (e.g., ```python). Lists and Indentation : Use bullet points or numbered lists to organize information clearly. Bold and Italics : Highlight key terms and important points using bold or italics . Tables : For comparing options or presenting structured data, use Markdown tables. Spacing : Add blank lines between sections and elements to prevent clutter and improve readability.","title":"Visual Enhancements"},{"location":"Pytest-Fixer-TDD-Blueprint/","text":"Pytest-Fixer TDD Blueprint Aligning on the Goal \ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f Let\u2019s adopt a fully Test-Driven Development (TDD) approach. This means we define our desired behavior and outcomes as tests first\u2014no production code will be written until we have tests describing what we want. Below is an outline of how we can start from scratch, writing tests for the core functionality of the pytest-fixer project. After we agree on and finalize these tests (the contract of what we want), we will proceed to implement the code that makes these tests pass. Key Principles 1. Test-Driven Development (TDD) Write tests that define the desired functionality and behavior. Run tests and see them fail. Write just enough code to make tests pass. Refactor as needed, keeping tests green. 2. Domain-Driven & Layered Architecture As previously discussed, we aim for a clean architecture (domain, application, infrastructure). We\u2019ll start simple: - Initial Tests : Focus on core domain logic and application-level use cases. - Add Complexity : Incrementally enhance the architecture as we progress. 3. Incremental Approach Start Simple : Begin with the simplest domain behaviors (e.g., managing TestError aggregates, fix attempts). Expand Outward : Move to application services (e.g., attempting a fix) and then to integration with AIManager , TestRunner , and ChangeApplier . Repeat Cycle : For each step, write tests first, then code. What We Want to Achieve Core User Story As a developer, I want the pytest-fixer tool to: 1. Identify test failures from pytest output. 2. Store them . 3. Attempt to fix them by: - Generating fixes with AI. - Applying changes to the code. - Verifying if the fix resolves the test failure. - If it fails, revert changes and try again with increased AI \u201ctemperature\u201d. - If it succeeds, mark the error as fixed. We will break this story into smaller, testable chunks. Project Structure We\u2019ll plan tests first. A suggested structure: pytest_fixer/ \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_domain_models.py \u2502 \u251c\u2500\u2500 test_error_analysis_service.py \u2502 \u251c\u2500\u2500 test_application_usecases.py \u2502 \u251c\u2500\u2500 test_integration.py \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 src/ \u251c\u2500\u2500 domain/ \u251c\u2500\u2500 application/ \u251c\u2500\u2500 infrastructure/ \u2514\u2500\u2500 ... Tests Directory ( tests/ ) : All tests reside here. Source Directory ( src/ ) : Future code will be placed here. Currently, only tests are written; no code exists in src/ yet. Step 1: Domain Model Tests Goal : Ensure our TestError and FixAttempt domain models behave correctly. Confirm that we can create TestError aggregates, add fix attempts, and mark them as fixed or failed. tests/test_domain_models.py import unittest from uuid import UUID class TestDomainModels(unittest.TestCase): def test_create_test_error(self): # We want to create a TestError with file, function, error details # We expect an unfixed status initially # Pseudocode usage: # error = TestError( # test_file=Path(\"tests/test_example.py\"), # test_function=\"test_something\", # error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") # ) # self.assertEqual(error.status, \"unfixed\") # self.assertEqual(error.test_function, \"test_something\") # self.assertIsNotNone(error.id) # Initially, this test will fail because we have no such classes implemented. # We'll just write the asserts we want: from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) self.assertEqual(error.status, \"unfixed\") self.assertEqual(error.test_function, \"test_something\") self.assertTrue(isinstance(error.id, UUID)) self.assertEqual(error.error_details.error_type, \"AssertionError\") self.assertEqual(error.error_details.message, \"Expected X but got Y\") def test_start_fix_attempt(self): # We want to start a fix attempt with a given temperature and see that attempt recorded from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) self.assertEqual(attempt.attempt_number, 1) self.assertEqual(attempt.temperature, 0.4) self.assertIn(attempt, error.fix_attempts) self.assertEqual(error.status, \"unfixed\") # still unfixed until success def test_mark_fixed(self): # After a successful fix, error should be \"fixed\" from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) # Pseudocode for success marking: # error.mark_fixed(attempt) # self.assertEqual(error.status, \"fixed\") # self.assertEqual(attempt.status, \"success\") error.mark_fixed(attempt) self.assertEqual(error.status, \"fixed\") self.assertEqual(attempt.status, \"success\") def test_mark_attempt_failed(self): from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) # If the attempt fails: error.mark_attempt_failed(attempt) self.assertEqual(attempt.status, \"failed\") self.assertEqual(error.status, \"unfixed\") # still unfixed after a failed attempt Result : Running these tests now would fail since src.domain.models doesn\u2019t exist. Step 2: Error Analysis Service Tests Goal : Create a service that, given pytest output, returns a list of TestError objects. Define a minimal test to ensure we can parse a known failing test from a snippet of pytest output. tests/test_error_analysis_service.py import unittest class TestErrorAnalysisService(unittest.TestCase): def test_analyze_simple_failure(self): # Given a simplified pytest output snippet: pytest_output = \"\"\" tests/test_example.py::test_something FAILED AssertionError: Expected X but got Y ----------------------------- stack trace details here \"\"\" # We expect the service to return a list with one TestError from src.domain.services import ErrorAnalysisService from pathlib import Path service = ErrorAnalysisService() errors = service.analyze_errors(pytest_output) self.assertIsNotNone(errors) self.assertEqual(len(errors), 1) error = errors[0] self.assertEqual(error.test_file, Path(\"tests/test_example.py\")) self.assertEqual(error.test_function, \"test_something\") self.assertEqual(error.error_details.error_type, \"AssertionError\") self.assertIn(\"Expected X but got Y\", error.error_details.message) Note : This test defines what we expect from ErrorAnalysisService , with no code for it yet. Step 3: Application Use Cases Tests Goal : Define a test for the main use case\u2014attempting to fix an unfixed error using a TestFixingService in the application layer. This service will: Retrieve an error by ID. Attempt to generate a fix using AIManager . Apply changes, verify fix using TestRunner . If successful, mark as fixed and commit with VCSManager . If failed, revert changes and retry until max_retries is reached. We will mock dependencies ( AIManager , TestRunner , VCSManager , ChangeApplier ) since we focus on logic rather than actual integration. tests/test_application_usecases.py import unittest from unittest.mock import MagicMock from uuid import uuid4 class TestApplicationUseCases(unittest.TestCase): def test_attempt_fix_success_on_first_try(self): # Setup a mock error repository with one unfixed error from src.domain.models import TestError, ErrorDetails from pathlib import Path error_id = uuid4() test_error = TestError( id=error_id, test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) mock_repo = MagicMock() mock_repo.get_by_id.return_value = test_error mock_repo.get_unfixed_errors.return_value = [test_error] # Mock AIManager to always return a CodeChanges object: from src.domain.models import CodeChanges mock_ai = MagicMock() mock_ai.generate_fix.return_value = CodeChanges(original=\"bug\", modified=\"fix\") # Mock TestRunner: run_test_and_check returns True on first attempt mock_test_runner = MagicMock() mock_test_runner.run_test_and_check.return_value = True # Mock VCSManager: just commit without error mock_vcs = MagicMock() # Mock ChangeApplier: apply and revert do nothing mock_applier = MagicMock() # Now test the service from src.application.usecases import TestFixingService service = TestFixingService( error_repo=mock_repo, ai_manager=mock_ai, test_runner=mock_test_runner, vcs_manager=mock_vcs, change_applier=mock_applier, initial_temp=0.4, temp_increment=0.1, max_retries=3 ) # Attempt fix success = service.attempt_fix(error_id) self.assertTrue(success) self.assertEqual(test_error.status, \"fixed\") # Ensure commit was called mock_vcs.commit_changes.assert_called_once() # Ensure test was run mock_test_runner.run_test_and_check.assert_called_once_with(test_error.test_file, test_error.test_function) # Ensure AI fix generated mock_ai.generate_fix.assert_called_once_with(test_error, 0.4) def test_attempt_fix_failure_all_retries(self): # If the fix never passes verification, we end up returning False from src.domain.models import TestError, ErrorDetails from pathlib import Path error_id = uuid4() test_error = TestError( id=error_id, test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) mock_repo = MagicMock() mock_repo.get_by_id.return_value = test_error # AI returns changes each time, but test never passes: from src.domain.models import CodeChanges mock_ai = MagicMock() mock_ai.generate_fix.return_value = CodeChanges(original=\"bug\", modified=\"fix\") mock_test_runner = MagicMock() mock_test_runner.run_test_and_check.return_value = False # never passes mock_vcs = MagicMock() mock_applier = MagicMock() from src.application.usecases import TestFixingService service = TestFixingService( error_repo=mock_repo, ai_manager=mock_ai, test_runner=mock_test_runner, vcs_manager=mock_vcs, change_applier=mock_applier, initial_temp=0.4, temp_increment=0.1, max_retries=2 ) success = service.attempt_fix(error_id) self.assertFalse(success) self.assertEqual(test_error.status, \"unfixed\") # Verifications: # AI generate fix should be called twice (max_retries=2) self.assertEqual(mock_ai.generate_fix.call_count, 2) # Test runner also called twice self.assertEqual(mock_test_runner.run_test_and_check.call_count, 2) # VCS commit never called mock_vcs.commit_changes.assert_not_called() # After each failure, revert should be called self.assertEqual(mock_applier.revert.call_count, 2) Step 4: Integration Test (Optional at this Stage) We could write a high-level test simulating the whole pipeline once we have some code. However, for now, these unit tests are sufficient to guide our initial implementation. Summary of Next Steps Run These Tests Now : They will fail because none of the referenced classes or logic exists. Implement Minimal Code in src/ : Develop just enough code to make these tests pass, step by step. Refactor the Code Once Tests Are Passing : Improve the code quality while ensuring tests remain green. We have a clear contract defined by tests, ensuring we only build what\u2019s required and verifying functionality as we proceed. This test suite and approach should serve as a strong starting point for a TDD-driven rewrite of the pytest-fixer tool\u2019s core functionality. Conclusion Adopting a Test-Driven Development approach ensures that our development process is guided by well-defined tests, promoting high-quality, maintainable, and reliable code. By following this blueprint, the pytest-fixer project will be built incrementally with a strong foundation, allowing for scalable and efficient development.","title":"Pytest Fixer TDD Blueprint"},{"location":"Pytest-Fixer-TDD-Blueprint/#pytest-fixer-tdd-blueprint","text":"","title":"Pytest-Fixer TDD Blueprint"},{"location":"Pytest-Fixer-TDD-Blueprint/#aligning-on-the-goal","text":"Let\u2019s adopt a fully Test-Driven Development (TDD) approach. This means we define our desired behavior and outcomes as tests first\u2014no production code will be written until we have tests describing what we want. Below is an outline of how we can start from scratch, writing tests for the core functionality of the pytest-fixer project. After we agree on and finalize these tests (the contract of what we want), we will proceed to implement the code that makes these tests pass.","title":"Aligning on the Goal \ud83e\uddd9\ud83c\udffe\u200d\u2642\ufe0f"},{"location":"Pytest-Fixer-TDD-Blueprint/#key-principles","text":"","title":"Key Principles"},{"location":"Pytest-Fixer-TDD-Blueprint/#1-test-driven-development-tdd","text":"Write tests that define the desired functionality and behavior. Run tests and see them fail. Write just enough code to make tests pass. Refactor as needed, keeping tests green.","title":"1. Test-Driven Development (TDD)"},{"location":"Pytest-Fixer-TDD-Blueprint/#2-domain-driven-layered-architecture","text":"As previously discussed, we aim for a clean architecture (domain, application, infrastructure). We\u2019ll start simple: - Initial Tests : Focus on core domain logic and application-level use cases. - Add Complexity : Incrementally enhance the architecture as we progress.","title":"2. Domain-Driven &amp; Layered Architecture"},{"location":"Pytest-Fixer-TDD-Blueprint/#3-incremental-approach","text":"Start Simple : Begin with the simplest domain behaviors (e.g., managing TestError aggregates, fix attempts). Expand Outward : Move to application services (e.g., attempting a fix) and then to integration with AIManager , TestRunner , and ChangeApplier . Repeat Cycle : For each step, write tests first, then code.","title":"3. Incremental Approach"},{"location":"Pytest-Fixer-TDD-Blueprint/#what-we-want-to-achieve","text":"","title":"What We Want to Achieve"},{"location":"Pytest-Fixer-TDD-Blueprint/#core-user-story","text":"As a developer, I want the pytest-fixer tool to: 1. Identify test failures from pytest output. 2. Store them . 3. Attempt to fix them by: - Generating fixes with AI. - Applying changes to the code. - Verifying if the fix resolves the test failure. - If it fails, revert changes and try again with increased AI \u201ctemperature\u201d. - If it succeeds, mark the error as fixed. We will break this story into smaller, testable chunks.","title":"Core User Story"},{"location":"Pytest-Fixer-TDD-Blueprint/#project-structure","text":"We\u2019ll plan tests first. A suggested structure: pytest_fixer/ \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_domain_models.py \u2502 \u251c\u2500\u2500 test_error_analysis_service.py \u2502 \u251c\u2500\u2500 test_application_usecases.py \u2502 \u251c\u2500\u2500 test_integration.py \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 src/ \u251c\u2500\u2500 domain/ \u251c\u2500\u2500 application/ \u251c\u2500\u2500 infrastructure/ \u2514\u2500\u2500 ... Tests Directory ( tests/ ) : All tests reside here. Source Directory ( src/ ) : Future code will be placed here. Currently, only tests are written; no code exists in src/ yet.","title":"Project Structure"},{"location":"Pytest-Fixer-TDD-Blueprint/#step-1-domain-model-tests","text":"Goal : Ensure our TestError and FixAttempt domain models behave correctly. Confirm that we can create TestError aggregates, add fix attempts, and mark them as fixed or failed.","title":"Step 1: Domain Model Tests"},{"location":"Pytest-Fixer-TDD-Blueprint/#teststest_domain_modelspy","text":"import unittest from uuid import UUID class TestDomainModels(unittest.TestCase): def test_create_test_error(self): # We want to create a TestError with file, function, error details # We expect an unfixed status initially # Pseudocode usage: # error = TestError( # test_file=Path(\"tests/test_example.py\"), # test_function=\"test_something\", # error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") # ) # self.assertEqual(error.status, \"unfixed\") # self.assertEqual(error.test_function, \"test_something\") # self.assertIsNotNone(error.id) # Initially, this test will fail because we have no such classes implemented. # We'll just write the asserts we want: from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) self.assertEqual(error.status, \"unfixed\") self.assertEqual(error.test_function, \"test_something\") self.assertTrue(isinstance(error.id, UUID)) self.assertEqual(error.error_details.error_type, \"AssertionError\") self.assertEqual(error.error_details.message, \"Expected X but got Y\") def test_start_fix_attempt(self): # We want to start a fix attempt with a given temperature and see that attempt recorded from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) self.assertEqual(attempt.attempt_number, 1) self.assertEqual(attempt.temperature, 0.4) self.assertIn(attempt, error.fix_attempts) self.assertEqual(error.status, \"unfixed\") # still unfixed until success def test_mark_fixed(self): # After a successful fix, error should be \"fixed\" from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) # Pseudocode for success marking: # error.mark_fixed(attempt) # self.assertEqual(error.status, \"fixed\") # self.assertEqual(attempt.status, \"success\") error.mark_fixed(attempt) self.assertEqual(error.status, \"fixed\") self.assertEqual(attempt.status, \"success\") def test_mark_attempt_failed(self): from pathlib import Path from src.domain.models import TestError, ErrorDetails error = TestError( test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) attempt = error.start_fix_attempt(0.4) # If the attempt fails: error.mark_attempt_failed(attempt) self.assertEqual(attempt.status, \"failed\") self.assertEqual(error.status, \"unfixed\") # still unfixed after a failed attempt Result : Running these tests now would fail since src.domain.models doesn\u2019t exist.","title":"tests/test_domain_models.py"},{"location":"Pytest-Fixer-TDD-Blueprint/#step-2-error-analysis-service-tests","text":"Goal : Create a service that, given pytest output, returns a list of TestError objects. Define a minimal test to ensure we can parse a known failing test from a snippet of pytest output.","title":"Step 2: Error Analysis Service Tests"},{"location":"Pytest-Fixer-TDD-Blueprint/#teststest_error_analysis_servicepy","text":"import unittest class TestErrorAnalysisService(unittest.TestCase): def test_analyze_simple_failure(self): # Given a simplified pytest output snippet: pytest_output = \"\"\" tests/test_example.py::test_something FAILED AssertionError: Expected X but got Y ----------------------------- stack trace details here \"\"\" # We expect the service to return a list with one TestError from src.domain.services import ErrorAnalysisService from pathlib import Path service = ErrorAnalysisService() errors = service.analyze_errors(pytest_output) self.assertIsNotNone(errors) self.assertEqual(len(errors), 1) error = errors[0] self.assertEqual(error.test_file, Path(\"tests/test_example.py\")) self.assertEqual(error.test_function, \"test_something\") self.assertEqual(error.error_details.error_type, \"AssertionError\") self.assertIn(\"Expected X but got Y\", error.error_details.message) Note : This test defines what we expect from ErrorAnalysisService , with no code for it yet.","title":"tests/test_error_analysis_service.py"},{"location":"Pytest-Fixer-TDD-Blueprint/#step-3-application-use-cases-tests","text":"Goal : Define a test for the main use case\u2014attempting to fix an unfixed error using a TestFixingService in the application layer. This service will: Retrieve an error by ID. Attempt to generate a fix using AIManager . Apply changes, verify fix using TestRunner . If successful, mark as fixed and commit with VCSManager . If failed, revert changes and retry until max_retries is reached. We will mock dependencies ( AIManager , TestRunner , VCSManager , ChangeApplier ) since we focus on logic rather than actual integration.","title":"Step 3: Application Use Cases Tests"},{"location":"Pytest-Fixer-TDD-Blueprint/#teststest_application_usecasespy","text":"import unittest from unittest.mock import MagicMock from uuid import uuid4 class TestApplicationUseCases(unittest.TestCase): def test_attempt_fix_success_on_first_try(self): # Setup a mock error repository with one unfixed error from src.domain.models import TestError, ErrorDetails from pathlib import Path error_id = uuid4() test_error = TestError( id=error_id, test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) mock_repo = MagicMock() mock_repo.get_by_id.return_value = test_error mock_repo.get_unfixed_errors.return_value = [test_error] # Mock AIManager to always return a CodeChanges object: from src.domain.models import CodeChanges mock_ai = MagicMock() mock_ai.generate_fix.return_value = CodeChanges(original=\"bug\", modified=\"fix\") # Mock TestRunner: run_test_and_check returns True on first attempt mock_test_runner = MagicMock() mock_test_runner.run_test_and_check.return_value = True # Mock VCSManager: just commit without error mock_vcs = MagicMock() # Mock ChangeApplier: apply and revert do nothing mock_applier = MagicMock() # Now test the service from src.application.usecases import TestFixingService service = TestFixingService( error_repo=mock_repo, ai_manager=mock_ai, test_runner=mock_test_runner, vcs_manager=mock_vcs, change_applier=mock_applier, initial_temp=0.4, temp_increment=0.1, max_retries=3 ) # Attempt fix success = service.attempt_fix(error_id) self.assertTrue(success) self.assertEqual(test_error.status, \"fixed\") # Ensure commit was called mock_vcs.commit_changes.assert_called_once() # Ensure test was run mock_test_runner.run_test_and_check.assert_called_once_with(test_error.test_file, test_error.test_function) # Ensure AI fix generated mock_ai.generate_fix.assert_called_once_with(test_error, 0.4) def test_attempt_fix_failure_all_retries(self): # If the fix never passes verification, we end up returning False from src.domain.models import TestError, ErrorDetails from pathlib import Path error_id = uuid4() test_error = TestError( id=error_id, test_file=Path(\"tests/test_example.py\"), test_function=\"test_something\", error_details=ErrorDetails(error_type=\"AssertionError\", message=\"Expected X but got Y\") ) mock_repo = MagicMock() mock_repo.get_by_id.return_value = test_error # AI returns changes each time, but test never passes: from src.domain.models import CodeChanges mock_ai = MagicMock() mock_ai.generate_fix.return_value = CodeChanges(original=\"bug\", modified=\"fix\") mock_test_runner = MagicMock() mock_test_runner.run_test_and_check.return_value = False # never passes mock_vcs = MagicMock() mock_applier = MagicMock() from src.application.usecases import TestFixingService service = TestFixingService( error_repo=mock_repo, ai_manager=mock_ai, test_runner=mock_test_runner, vcs_manager=mock_vcs, change_applier=mock_applier, initial_temp=0.4, temp_increment=0.1, max_retries=2 ) success = service.attempt_fix(error_id) self.assertFalse(success) self.assertEqual(test_error.status, \"unfixed\") # Verifications: # AI generate fix should be called twice (max_retries=2) self.assertEqual(mock_ai.generate_fix.call_count, 2) # Test runner also called twice self.assertEqual(mock_test_runner.run_test_and_check.call_count, 2) # VCS commit never called mock_vcs.commit_changes.assert_not_called() # After each failure, revert should be called self.assertEqual(mock_applier.revert.call_count, 2)","title":"tests/test_application_usecases.py"},{"location":"Pytest-Fixer-TDD-Blueprint/#step-4-integration-test-optional-at-this-stage","text":"We could write a high-level test simulating the whole pipeline once we have some code. However, for now, these unit tests are sufficient to guide our initial implementation.","title":"Step 4: Integration Test (Optional at this Stage)"},{"location":"Pytest-Fixer-TDD-Blueprint/#summary-of-next-steps","text":"Run These Tests Now : They will fail because none of the referenced classes or logic exists. Implement Minimal Code in src/ : Develop just enough code to make these tests pass, step by step. Refactor the Code Once Tests Are Passing : Improve the code quality while ensuring tests remain green. We have a clear contract defined by tests, ensuring we only build what\u2019s required and verifying functionality as we proceed. This test suite and approach should serve as a strong starting point for a TDD-driven rewrite of the pytest-fixer tool\u2019s core functionality.","title":"Summary of Next Steps"},{"location":"Pytest-Fixer-TDD-Blueprint/#conclusion","text":"Adopting a Test-Driven Development approach ensures that our development process is guided by well-defined tests, promoting high-quality, maintainable, and reliable code. By following this blueprint, the pytest-fixer project will be built incrementally with a strong foundation, allowing for scalable and efficient development.","title":"Conclusion"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/","text":"Repository Structure and Architecture Design Proposal This document outlines the proposed repository structure for the pytest_fixer project, aligning with Domain-Driven Design (DDD) and Clean Architecture principles. It includes recommendations for optimizing the structure for maintainability, scalability, and clarity. Proposed Repository Structure pytest_fixer/ \u251c\u2500\u2500 domain/ \u2502 \u251c\u2500\u2500 models.py # Entities, Value Objects \u2502 \u251c\u2500\u2500 events.py # Domain events \u2502 \u251c\u2500\u2500 repositories.py # Repository interfaces \u2502 \u251c\u2500\u2500 services.py # Domain services (e.g., ErrorAnalysisService) \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 application/ \u2502 \u251c\u2500\u2500 usecases.py # Application services (Use cases) \u2502 \u251c\u2500\u2500 dto.py # Data Transfer Objects, if needed \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 ai_manager.py # AI integration (fix generation) \u2502 \u251c\u2500\u2500 test_runner.py # Pytest integration \u2502 \u251c\u2500\u2500 vcs_manager.py # Git operations \u2502 \u251c\u2500\u2500 repository_impl.py# Repository implementations \u2502 \u251c\u2500\u2500 change_applier.py # Applying and reverting code changes \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_domain_models.py \u2502 \u251c\u2500\u2500 test_error_analysis_service.py \u2502 \u251c\u2500\u2500 test_application_usecases.py \u2502 \u251c\u2500\u2500 test_integration.py \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 settings.py # Configuration settings \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 scripts/ \u2502 \u251c\u2500\u2500 setup_env.sh # Environment setup script \u2502 \u2514\u2500\u2500 migrate.py # Database migration script, if needed \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 architecture.md # Architectural documentation \u2502 \u2514\u2500\u2500 user_guide.md # User guide and tutorials \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 main.py \u251c\u2500\u2500 README.md \u2514\u2500\u2500 .github/ \u2514\u2500\u2500 workflows/ \u2514\u2500\u2500 ci.yml # Continuous Integration configuration Layered Structure Domain Layer Core business logic: Entities, value objects, domain services, and repository interfaces. Contains files like: models.py , services.py , and repositories.py . Application Layer Manages use cases and coordinates between the domain and infrastructure layers. Contains files like: usecases.py and dto.py . Infrastructure Layer Implements functionalities such as AI integration, test running, and version control. Contains files like: ai_manager.py , test_runner.py , and vcs_manager.py . Tests Directory Organized to ensure comprehensive test coverage for all layers. Recommendations 1. Separate Concerns Clearly Keep the domain, application, and infrastructure layers distinct. Maintain test files corresponding to each layer in the tests/ directory. 2. Add a config Module Use a dedicated config/ directory for environment settings and configurations. Example: pytest_fixer/ \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 settings.py # Configuration settings \u2502 \u2514\u2500\u2500 __init__.py 3. Logging Setup Include a logger.py file in the infrastructure/ layer for centralized logging. Example: infrastructure/ \u251c\u2500\u2500 logger.py # Logging setup 4. Scripts and Utilities Add a scripts/ directory for utility scripts like migrations and environment setup. Example: pytest_fixer/ \u251c\u2500\u2500 scripts/ \u2502 \u251c\u2500\u2500 setup_env.sh # Environment setup script \u2502 \u2514\u2500\u2500 migrate.py # Database migration script 5. Documentation Use a docs/ directory to maintain architectural documentation and user guides. Example: pytest_fixer/ \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 architecture.md \u2502 \u2514\u2500\u2500 user_guide.md 6. Virtual Environment and Dependencies Ensure .venv/ is excluded in .gitignore . Manage dependencies using requirements.txt or Pipfile . 7. Continuous Integration (CI) Include CI configurations for automated testing and deployment in .github/workflows/ . Example: pytest_fixer/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 ci.yml # Continuous Integration configuration 8. Main Entry Point Use main.py as the starting point for service initialization and orchestration. Final Thoughts This structure, based on DDD and Clean Architecture, provides a robust foundation for your project. Implementing these recommendations incrementally will ensure: Clear separation of concerns. Easy onboarding for new contributors. Scalable and maintainable code. Feel free to adapt the structure and suggestions to suit your specific needs. Let me know if you\u2019d like further refinements!","title":"Repository Structure and Architecture Design Proposal"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#repository-structure-and-architecture-design-proposal","text":"This document outlines the proposed repository structure for the pytest_fixer project, aligning with Domain-Driven Design (DDD) and Clean Architecture principles. It includes recommendations for optimizing the structure for maintainability, scalability, and clarity.","title":"Repository Structure and Architecture Design Proposal"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#proposed-repository-structure","text":"pytest_fixer/ \u251c\u2500\u2500 domain/ \u2502 \u251c\u2500\u2500 models.py # Entities, Value Objects \u2502 \u251c\u2500\u2500 events.py # Domain events \u2502 \u251c\u2500\u2500 repositories.py # Repository interfaces \u2502 \u251c\u2500\u2500 services.py # Domain services (e.g., ErrorAnalysisService) \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 application/ \u2502 \u251c\u2500\u2500 usecases.py # Application services (Use cases) \u2502 \u251c\u2500\u2500 dto.py # Data Transfer Objects, if needed \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 ai_manager.py # AI integration (fix generation) \u2502 \u251c\u2500\u2500 test_runner.py # Pytest integration \u2502 \u251c\u2500\u2500 vcs_manager.py # Git operations \u2502 \u251c\u2500\u2500 repository_impl.py# Repository implementations \u2502 \u251c\u2500\u2500 change_applier.py # Applying and reverting code changes \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 test_domain_models.py \u2502 \u251c\u2500\u2500 test_error_analysis_service.py \u2502 \u251c\u2500\u2500 test_application_usecases.py \u2502 \u251c\u2500\u2500 test_integration.py \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 settings.py # Configuration settings \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 scripts/ \u2502 \u251c\u2500\u2500 setup_env.sh # Environment setup script \u2502 \u2514\u2500\u2500 migrate.py # Database migration script, if needed \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 architecture.md # Architectural documentation \u2502 \u2514\u2500\u2500 user_guide.md # User guide and tutorials \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 main.py \u251c\u2500\u2500 README.md \u2514\u2500\u2500 .github/ \u2514\u2500\u2500 workflows/ \u2514\u2500\u2500 ci.yml # Continuous Integration configuration","title":"Proposed Repository Structure"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#layered-structure","text":"Domain Layer Core business logic: Entities, value objects, domain services, and repository interfaces. Contains files like: models.py , services.py , and repositories.py . Application Layer Manages use cases and coordinates between the domain and infrastructure layers. Contains files like: usecases.py and dto.py . Infrastructure Layer Implements functionalities such as AI integration, test running, and version control. Contains files like: ai_manager.py , test_runner.py , and vcs_manager.py . Tests Directory Organized to ensure comprehensive test coverage for all layers.","title":"Layered Structure"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#recommendations","text":"","title":"Recommendations"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#1-separate-concerns-clearly","text":"Keep the domain, application, and infrastructure layers distinct. Maintain test files corresponding to each layer in the tests/ directory.","title":"1. Separate Concerns Clearly"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#2-add-a-config-module","text":"Use a dedicated config/ directory for environment settings and configurations. Example: pytest_fixer/ \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 settings.py # Configuration settings \u2502 \u2514\u2500\u2500 __init__.py","title":"2. Add a config Module"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#3-logging-setup","text":"Include a logger.py file in the infrastructure/ layer for centralized logging. Example: infrastructure/ \u251c\u2500\u2500 logger.py # Logging setup","title":"3. Logging Setup"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#4-scripts-and-utilities","text":"Add a scripts/ directory for utility scripts like migrations and environment setup. Example: pytest_fixer/ \u251c\u2500\u2500 scripts/ \u2502 \u251c\u2500\u2500 setup_env.sh # Environment setup script \u2502 \u2514\u2500\u2500 migrate.py # Database migration script","title":"4. Scripts and Utilities"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#5-documentation","text":"Use a docs/ directory to maintain architectural documentation and user guides. Example: pytest_fixer/ \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 architecture.md \u2502 \u2514\u2500\u2500 user_guide.md","title":"5. Documentation"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#6-virtual-environment-and-dependencies","text":"Ensure .venv/ is excluded in .gitignore . Manage dependencies using requirements.txt or Pipfile .","title":"6. Virtual Environment and Dependencies"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#7-continuous-integration-ci","text":"Include CI configurations for automated testing and deployment in .github/workflows/ . Example: pytest_fixer/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 ci.yml # Continuous Integration configuration","title":"7. Continuous Integration (CI)"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#8-main-entry-point","text":"Use main.py as the starting point for service initialization and orchestration.","title":"8. Main Entry Point"},{"location":"Repository-Structure-and-Architecture-Design-Proposal/#final-thoughts","text":"This structure, based on DDD and Clean Architecture, provides a robust foundation for your project. Implementing these recommendations incrementally will ensure: Clear separation of concerns. Easy onboarding for new contributors. Scalable and maintainable code. Feel free to adapt the structure and suggestions to suit your specific needs. Let me know if you\u2019d like further refinements!","title":"Final Thoughts"},{"location":"pytest-fixer-User-%26-Developer-Guide/","text":"Here's a cleaned-up version of your document in pretty Markdown that maintains all the key features while improving readability and formatting: pytest-fixer Documentation Overview pytest-fixer is an AI-powered tool designed to automatically identify and fix failing pytest tests in Python projects. By combining OpenAI's GPT models with intelligent error analysis and Git-based change management, it provides a robust solution for testing automation. Core Architecture Component Structure PytestErrorFixer Orchestrates all components. Manages execution flow, state, initialization, and cleanup. TestRunner Discovers test files matching test_*.py . Executes individual tests or full test suites. Captures stdout , stderr , and results as standardized objects. ErrorProcessor Parses pytest output into structured data. Handles test failures, collection errors, and runtime issues. Extracts error context and details for fixing. DependencyMapper Analyzes imports and code references. Maps relationships between test and source files. Supports both relative and absolute imports. AIManager Manages OpenAI API interactions. Implements a temperature-based retry strategy. Constructs prompts and parses AI responses into actionable changes. StateManager Tracks execution progress and state. Handles checkpoints, recovery, and logs. ChangeHandler Applies, verifies, and reverts code changes. Manages logs and ensures smooth operations. Component Interaction graph TD TestRunner --> ErrorProcessor --> AIManager AIManager --> ChangeHandler ChangeHandler --> StateManager StateManager --> LogStore DependencyMapper --> PytestErrorFixer Key Features Test Processing Automatic test discovery (excludes .venv ). Execution of individual test functions or files. Dependency analysis and detailed error extraction. AI Integration OpenAI GPT model integration with retry mechanism: python initial_temperature = 0.4 temperature_increment = 0.1 max_retries = 3 Context-aware prompt construction , including: Error details and stack traces. Previous attempt history. Code context and dependencies. Test output and logs. Interactive Modes Operational modes for flexibility: - DISABLED : Fully automated. - ON_FAILURE : Interactive on failures. - ALWAYS : Always interactive. - MANUAL : Triggered manually. Interactive commands: - show : Display errors and changes. - edit : Open changes in the default editor. - apply : Apply and test changes. - retry : Retry with higher temperature. - prompt : Modify AI prompt. - diff : Show Git differences. - history : View change history. - quit : Exit without changes. State Management Tracks session progress and checkpoints. Logs state changes and error progression. Git Integration Creates isolated fix branches. Handles branch lifecycle and change management. Enables Git-based reversion and recovery. Setup Requirements Python 3.8+ Git OpenAI API Key pytest Installation git clone https://github.com/your-repo/pytest-fixer.git cd pytest-fixer pip install -r requirements.txt Configuration Create a .env file: OPENAI_API_KEY=your-api-key MODEL_NAME=gpt-4o-mini INITIAL_TEMPERATURE=0.4 TEMPERATURE_INCREMENT=0.1 MAX_RETRIES=3 Usage python -m pytest_fixer.main /path/to/project [options] Options: --initial-temperature FLOAT Initial AI temperature (default: 0.4) --temperature-increment FLOAT Temperature increment (default: 0.1) --max-retries INT Maximum fix attempts (default: 3) --model STRING AI model (default: gpt-4o-mini) --debug Enable debug logging --manual-fix Enable manual fix mode Error Handling Error Propagation Component-level errors : Managed within components. Errors logged and propagated. State preserved when possible. System-level errors : Managed by PytestErrorFixer . Trigger session cleanup while preserving logs. Recovery Mechanisms State Recovery : Checkpoint-based restoration. Log-based state reconstruction. Change Recovery : Git-based reversion. Preserves logs and session state. Known Limitations Technical Assumes \"main\" branch in Git. No parallel test execution. Basic pytest plugin support. OpenAI-specific AI integration. Operational Single-branch workflow. Limited error pattern recognition. No historical learning. Best Practices Version Control : Use a clean working directory. Create dedicated fix branches. Review all changes before merging. Configuration : Use default temperatures initially. Enable debug mode for investigation. Monitoring : Track progress logs and state changes. Troubleshooting : Verify API keys, Git permissions, and pytest setup. Support For assistance: 1. Enable debug logging. 2. Check logs in ~/.pytest_fixer/logs . 3. Include the following in issues: - Error details. - Logs. - Steps to reproduce. - Configuration details. This version is streamlined for readability while keeping technical details intact. It uses headings, lists, code blocks, and diagrams for clarity and organization. Let me know if you'd like further tweaks!","title":"User & Developer Guide"},{"location":"pytest-fixer-User-%26-Developer-Guide/#pytest-fixer-documentation","text":"","title":"pytest-fixer Documentation"},{"location":"pytest-fixer-User-%26-Developer-Guide/#overview","text":"pytest-fixer is an AI-powered tool designed to automatically identify and fix failing pytest tests in Python projects. By combining OpenAI's GPT models with intelligent error analysis and Git-based change management, it provides a robust solution for testing automation.","title":"Overview"},{"location":"pytest-fixer-User-%26-Developer-Guide/#core-architecture","text":"","title":"Core Architecture"},{"location":"pytest-fixer-User-%26-Developer-Guide/#component-structure","text":"PytestErrorFixer Orchestrates all components. Manages execution flow, state, initialization, and cleanup. TestRunner Discovers test files matching test_*.py . Executes individual tests or full test suites. Captures stdout , stderr , and results as standardized objects. ErrorProcessor Parses pytest output into structured data. Handles test failures, collection errors, and runtime issues. Extracts error context and details for fixing. DependencyMapper Analyzes imports and code references. Maps relationships between test and source files. Supports both relative and absolute imports. AIManager Manages OpenAI API interactions. Implements a temperature-based retry strategy. Constructs prompts and parses AI responses into actionable changes. StateManager Tracks execution progress and state. Handles checkpoints, recovery, and logs. ChangeHandler Applies, verifies, and reverts code changes. Manages logs and ensures smooth operations.","title":"Component Structure"},{"location":"pytest-fixer-User-%26-Developer-Guide/#component-interaction","text":"graph TD TestRunner --> ErrorProcessor --> AIManager AIManager --> ChangeHandler ChangeHandler --> StateManager StateManager --> LogStore DependencyMapper --> PytestErrorFixer","title":"Component Interaction"},{"location":"pytest-fixer-User-%26-Developer-Guide/#key-features","text":"","title":"Key Features"},{"location":"pytest-fixer-User-%26-Developer-Guide/#test-processing","text":"Automatic test discovery (excludes .venv ). Execution of individual test functions or files. Dependency analysis and detailed error extraction.","title":"Test Processing"},{"location":"pytest-fixer-User-%26-Developer-Guide/#ai-integration","text":"OpenAI GPT model integration with retry mechanism: python initial_temperature = 0.4 temperature_increment = 0.1 max_retries = 3 Context-aware prompt construction , including: Error details and stack traces. Previous attempt history. Code context and dependencies. Test output and logs.","title":"AI Integration"},{"location":"pytest-fixer-User-%26-Developer-Guide/#interactive-modes","text":"Operational modes for flexibility: - DISABLED : Fully automated. - ON_FAILURE : Interactive on failures. - ALWAYS : Always interactive. - MANUAL : Triggered manually. Interactive commands: - show : Display errors and changes. - edit : Open changes in the default editor. - apply : Apply and test changes. - retry : Retry with higher temperature. - prompt : Modify AI prompt. - diff : Show Git differences. - history : View change history. - quit : Exit without changes.","title":"Interactive Modes"},{"location":"pytest-fixer-User-%26-Developer-Guide/#state-management","text":"Tracks session progress and checkpoints. Logs state changes and error progression.","title":"State Management"},{"location":"pytest-fixer-User-%26-Developer-Guide/#git-integration","text":"Creates isolated fix branches. Handles branch lifecycle and change management. Enables Git-based reversion and recovery.","title":"Git Integration"},{"location":"pytest-fixer-User-%26-Developer-Guide/#setup","text":"","title":"Setup"},{"location":"pytest-fixer-User-%26-Developer-Guide/#requirements","text":"Python 3.8+ Git OpenAI API Key pytest","title":"Requirements"},{"location":"pytest-fixer-User-%26-Developer-Guide/#installation","text":"git clone https://github.com/your-repo/pytest-fixer.git cd pytest-fixer pip install -r requirements.txt","title":"Installation"},{"location":"pytest-fixer-User-%26-Developer-Guide/#configuration","text":"Create a .env file: OPENAI_API_KEY=your-api-key MODEL_NAME=gpt-4o-mini INITIAL_TEMPERATURE=0.4 TEMPERATURE_INCREMENT=0.1 MAX_RETRIES=3","title":"Configuration"},{"location":"pytest-fixer-User-%26-Developer-Guide/#usage","text":"python -m pytest_fixer.main /path/to/project [options] Options: --initial-temperature FLOAT Initial AI temperature (default: 0.4) --temperature-increment FLOAT Temperature increment (default: 0.1) --max-retries INT Maximum fix attempts (default: 3) --model STRING AI model (default: gpt-4o-mini) --debug Enable debug logging --manual-fix Enable manual fix mode","title":"Usage"},{"location":"pytest-fixer-User-%26-Developer-Guide/#error-handling","text":"","title":"Error Handling"},{"location":"pytest-fixer-User-%26-Developer-Guide/#error-propagation","text":"Component-level errors : Managed within components. Errors logged and propagated. State preserved when possible. System-level errors : Managed by PytestErrorFixer . Trigger session cleanup while preserving logs.","title":"Error Propagation"},{"location":"pytest-fixer-User-%26-Developer-Guide/#recovery-mechanisms","text":"State Recovery : Checkpoint-based restoration. Log-based state reconstruction. Change Recovery : Git-based reversion. Preserves logs and session state.","title":"Recovery Mechanisms"},{"location":"pytest-fixer-User-%26-Developer-Guide/#known-limitations","text":"","title":"Known Limitations"},{"location":"pytest-fixer-User-%26-Developer-Guide/#technical","text":"Assumes \"main\" branch in Git. No parallel test execution. Basic pytest plugin support. OpenAI-specific AI integration.","title":"Technical"},{"location":"pytest-fixer-User-%26-Developer-Guide/#operational","text":"Single-branch workflow. Limited error pattern recognition. No historical learning.","title":"Operational"},{"location":"pytest-fixer-User-%26-Developer-Guide/#best-practices","text":"Version Control : Use a clean working directory. Create dedicated fix branches. Review all changes before merging. Configuration : Use default temperatures initially. Enable debug mode for investigation. Monitoring : Track progress logs and state changes. Troubleshooting : Verify API keys, Git permissions, and pytest setup.","title":"Best Practices"},{"location":"pytest-fixer-User-%26-Developer-Guide/#support","text":"For assistance: 1. Enable debug logging. 2. Check logs in ~/.pytest_fixer/logs . 3. Include the following in issues: - Error details. - Logs. - Steps to reproduce. - Configuration details. This version is streamlined for readability while keeping technical details intact. It uses headings, lists, code blocks, and diagrams for clarity and organization. Let me know if you'd like further tweaks!","title":"Support"}]}