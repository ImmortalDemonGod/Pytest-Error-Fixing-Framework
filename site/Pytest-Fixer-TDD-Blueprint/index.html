<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Pytest Fixer TDD Blueprint - Pytest Error Fixing Framework</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Pytest Fixer TDD Blueprint";
        var mkdocs_page_input_path = "Pytest-Fixer-TDD-Blueprint.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Pytest Error Fixing Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/">DDD for Pytest Fixer Concepts and Implementation Guide</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Pytest Fixer TDD Blueprint</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#aligning-on-the-goal">Aligning on the Goal 🧙🏾‍♂️</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#key-principles">Key Principles</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-test-driven-development-tdd">1. Test-Driven Development (TDD)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-domain-driven-layered-architecture">2. Domain-Driven &amp; Layered Architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3-incremental-approach">3. Incremental Approach</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-we-want-to-achieve">What We Want to Achieve</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#core-user-story">Core User Story</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#project-structure">Project Structure</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#step-1-domain-model-tests">Step 1: Domain Model Tests</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#teststest_domain_modelspy">tests/test_domain_models.py</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#step-2-error-analysis-service-tests">Step 2: Error Analysis Service Tests</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#teststest_error_analysis_servicepy">tests/test_error_analysis_service.py</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#step-3-application-use-cases-tests">Step 3: Application Use Cases Tests</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#teststest_application_usecasespy">tests/test_application_usecases.py</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#step-4-integration-test-optional-at-this-stage">Step 4: Integration Test (Optional at this Stage)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#summary-of-next-steps">Summary of Next Steps</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Repository-Structure-and-Architecture-Design-Proposal/">Repository Structure and Architecture Design Proposal</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pytest-fixer-User-%26-Developer-Guide/">User & Developer Guide</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Pytest Error Fixing Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Pytest Fixer TDD Blueprint</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="pytest-fixer-tdd-blueprint">Pytest-Fixer TDD Blueprint</h1>
<h2 id="aligning-on-the-goal">Aligning on the Goal 🧙🏾‍♂️</h2>
<p>Let’s adopt a <strong>fully Test-Driven Development (TDD)</strong> approach. This means we define our desired behavior and outcomes as tests first—no production code will be written until we have tests describing what we want.</p>
<p>Below is an outline of how we can start from scratch, writing tests for the core functionality of the <code>pytest-fixer</code> project. After we agree on and finalize these tests (the contract of what we want), we will proceed to implement the code that makes these tests pass.</p>
<hr />
<h2 id="key-principles">Key Principles</h2>
<h3 id="1-test-driven-development-tdd">1. Test-Driven Development (TDD)</h3>
<ul>
<li><strong>Write tests</strong> that define the desired functionality and behavior.</li>
<li><strong>Run tests</strong> and see them fail.</li>
<li><strong>Write just enough code</strong> to make tests pass.</li>
<li><strong>Refactor</strong> as needed, keeping tests green.</li>
</ul>
<h3 id="2-domain-driven-layered-architecture">2. Domain-Driven &amp; Layered Architecture</h3>
<p>As previously discussed, we aim for a clean architecture (domain, application, infrastructure). We’ll start simple:
- <strong>Initial Tests</strong>: Focus on core domain logic and application-level use cases.
- <strong>Add Complexity</strong>: Incrementally enhance the architecture as we progress.</p>
<h3 id="3-incremental-approach">3. Incremental Approach</h3>
<ul>
<li><strong>Start Simple</strong>: Begin with the simplest domain behaviors (e.g., managing <code>TestError</code> aggregates, fix attempts).</li>
<li><strong>Expand Outward</strong>: Move to application services (e.g., attempting a fix) and then to integration with <code>AIManager</code>, <code>TestRunner</code>, and <code>ChangeApplier</code>.</li>
<li><strong>Repeat Cycle</strong>: For each step, write tests first, then code.</li>
</ul>
<hr />
<h2 id="what-we-want-to-achieve">What We Want to Achieve</h2>
<h3 id="core-user-story">Core User Story</h3>
<p>As a developer, I want the <code>pytest-fixer</code> tool to:
1. <strong>Identify test failures</strong> from pytest output.
2. <strong>Store them</strong>.
3. <strong>Attempt to fix them</strong> by:
   - Generating fixes with AI.
   - Applying changes to the code.
   - Verifying if the fix resolves the test failure.
   - If it fails, revert changes and try again with increased AI “temperature”.
   - If it succeeds, mark the error as fixed.</p>
<p>We will break this story into smaller, testable chunks.</p>
<hr />
<h2 id="project-structure">Project Structure</h2>
<p>We’ll plan tests first. A suggested structure:</p>
<pre><code>pytest_fixer/
├── tests/
│   ├── test_domain_models.py
│   ├── test_error_analysis_service.py
│   ├── test_application_usecases.py
│   ├── test_integration.py
│   └── __init__.py
└── src/
    ├── domain/
    ├── application/
    ├── infrastructure/
    └── ...
</code></pre>
<ul>
<li><strong>Tests Directory (<code>tests/</code>)</strong>: All tests reside here.</li>
<li><strong>Source Directory (<code>src/</code>)</strong>: Future code will be placed here. Currently, only tests are written; no code exists in <code>src/</code> yet.</li>
</ul>
<hr />
<h2 id="step-1-domain-model-tests">Step 1: Domain Model Tests</h2>
<p><strong>Goal</strong>: Ensure our <code>TestError</code> and <code>FixAttempt</code> domain models behave correctly. Confirm that we can create <code>TestError</code> aggregates, add fix attempts, and mark them as fixed or failed.</p>
<h3 id="teststest_domain_modelspy"><code>tests/test_domain_models.py</code></h3>
<pre><code class="language-python">import unittest
from uuid import UUID

class TestDomainModels(unittest.TestCase):
    def test_create_test_error(self):
        # We want to create a TestError with file, function, error details
        # We expect an unfixed status initially
        # Pseudocode usage:
        # error = TestError(
        #     test_file=Path(&quot;tests/test_example.py&quot;),
        #     test_function=&quot;test_something&quot;,
        #     error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        # )
        # self.assertEqual(error.status, &quot;unfixed&quot;)
        # self.assertEqual(error.test_function, &quot;test_something&quot;)
        # self.assertIsNotNone(error.id)

        # Initially, this test will fail because we have no such classes implemented.
        # We'll just write the asserts we want:
        from pathlib import Path
        from src.domain.models import TestError, ErrorDetails

        error = TestError(
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )

        self.assertEqual(error.status, &quot;unfixed&quot;)
        self.assertEqual(error.test_function, &quot;test_something&quot;)
        self.assertTrue(isinstance(error.id, UUID))
        self.assertEqual(error.error_details.error_type, &quot;AssertionError&quot;)
        self.assertEqual(error.error_details.message, &quot;Expected X but got Y&quot;)

    def test_start_fix_attempt(self):
        # We want to start a fix attempt with a given temperature and see that attempt recorded
        from pathlib import Path
        from src.domain.models import TestError, ErrorDetails

        error = TestError(
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )

        attempt = error.start_fix_attempt(0.4)
        self.assertEqual(attempt.attempt_number, 1)
        self.assertEqual(attempt.temperature, 0.4)
        self.assertIn(attempt, error.fix_attempts)
        self.assertEqual(error.status, &quot;unfixed&quot;)  # still unfixed until success

    def test_mark_fixed(self):
        # After a successful fix, error should be &quot;fixed&quot;
        from pathlib import Path
        from src.domain.models import TestError, ErrorDetails

        error = TestError(
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )
        attempt = error.start_fix_attempt(0.4)
        # Pseudocode for success marking:
        # error.mark_fixed(attempt)
        # self.assertEqual(error.status, &quot;fixed&quot;)
        # self.assertEqual(attempt.status, &quot;success&quot;)

        error.mark_fixed(attempt)
        self.assertEqual(error.status, &quot;fixed&quot;)
        self.assertEqual(attempt.status, &quot;success&quot;)

    def test_mark_attempt_failed(self):
        from pathlib import Path
        from src.domain.models import TestError, ErrorDetails

        error = TestError(
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )
        attempt = error.start_fix_attempt(0.4)

        # If the attempt fails:
        error.mark_attempt_failed(attempt)
        self.assertEqual(attempt.status, &quot;failed&quot;)
        self.assertEqual(error.status, &quot;unfixed&quot;)  # still unfixed after a failed attempt
</code></pre>
<p><strong>Result</strong>: Running these tests now would fail since <code>src.domain.models</code> doesn’t exist.</p>
<hr />
<h2 id="step-2-error-analysis-service-tests">Step 2: Error Analysis Service Tests</h2>
<p><strong>Goal</strong>: Create a service that, given pytest output, returns a list of <code>TestError</code> objects. Define a minimal test to ensure we can parse a known failing test from a snippet of pytest output.</p>
<h3 id="teststest_error_analysis_servicepy"><code>tests/test_error_analysis_service.py</code></h3>
<pre><code class="language-python">import unittest

class TestErrorAnalysisService(unittest.TestCase):
    def test_analyze_simple_failure(self):
        # Given a simplified pytest output snippet:
        pytest_output = &quot;&quot;&quot;
        tests/test_example.py::test_something FAILED AssertionError: Expected X but got Y
        -----------------------------
        stack trace details here
        &quot;&quot;&quot;

        # We expect the service to return a list with one TestError
        from src.domain.services import ErrorAnalysisService
        from pathlib import Path
        service = ErrorAnalysisService()

        errors = service.analyze_errors(pytest_output)
        self.assertIsNotNone(errors)
        self.assertEqual(len(errors), 1)
        error = errors[0]
        self.assertEqual(error.test_file, Path(&quot;tests/test_example.py&quot;))
        self.assertEqual(error.test_function, &quot;test_something&quot;)
        self.assertEqual(error.error_details.error_type, &quot;AssertionError&quot;)
        self.assertIn(&quot;Expected X but got Y&quot;, error.error_details.message)
</code></pre>
<p><em>Note</em>: This test defines what we expect from <code>ErrorAnalysisService</code>, with no code for it yet.</p>
<hr />
<h2 id="step-3-application-use-cases-tests">Step 3: Application Use Cases Tests</h2>
<p><strong>Goal</strong>: Define a test for the main use case—attempting to fix an unfixed error using a <code>TestFixingService</code> in the application layer. This service will:</p>
<ul>
<li>Retrieve an error by ID.</li>
<li>Attempt to generate a fix using <code>AIManager</code>.</li>
<li>Apply changes, verify fix using <code>TestRunner</code>.</li>
<li>If successful, mark as fixed and commit with <code>VCSManager</code>.</li>
<li>If failed, revert changes and retry until <code>max_retries</code> is reached.</li>
</ul>
<p>We will <strong>mock dependencies</strong> (<code>AIManager</code>, <code>TestRunner</code>, <code>VCSManager</code>, <code>ChangeApplier</code>) since we focus on logic rather than actual integration.</p>
<h3 id="teststest_application_usecasespy"><code>tests/test_application_usecases.py</code></h3>
<pre><code class="language-python">import unittest
from unittest.mock import MagicMock
from uuid import uuid4

class TestApplicationUseCases(unittest.TestCase):
    def test_attempt_fix_success_on_first_try(self):
        # Setup a mock error repository with one unfixed error
        from src.domain.models import TestError, ErrorDetails
        from pathlib import Path
        error_id = uuid4()
        test_error = TestError(
            id=error_id,
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )

        mock_repo = MagicMock()
        mock_repo.get_by_id.return_value = test_error
        mock_repo.get_unfixed_errors.return_value = [test_error]

        # Mock AIManager to always return a CodeChanges object:
        from src.domain.models import CodeChanges
        mock_ai = MagicMock()
        mock_ai.generate_fix.return_value = CodeChanges(original=&quot;bug&quot;, modified=&quot;fix&quot;)

        # Mock TestRunner: run_test_and_check returns True on first attempt
        mock_test_runner = MagicMock()
        mock_test_runner.run_test_and_check.return_value = True

        # Mock VCSManager: just commit without error
        mock_vcs = MagicMock()

        # Mock ChangeApplier: apply and revert do nothing
        mock_applier = MagicMock()

        # Now test the service
        from src.application.usecases import TestFixingService
        service = TestFixingService(
            error_repo=mock_repo,
            ai_manager=mock_ai,
            test_runner=mock_test_runner,
            vcs_manager=mock_vcs,
            change_applier=mock_applier,
            initial_temp=0.4,
            temp_increment=0.1,
            max_retries=3
        )

        # Attempt fix
        success = service.attempt_fix(error_id)
        self.assertTrue(success)
        self.assertEqual(test_error.status, &quot;fixed&quot;)
        # Ensure commit was called
        mock_vcs.commit_changes.assert_called_once()
        # Ensure test was run
        mock_test_runner.run_test_and_check.assert_called_once_with(test_error.test_file, test_error.test_function)
        # Ensure AI fix generated
        mock_ai.generate_fix.assert_called_once_with(test_error, 0.4)

    def test_attempt_fix_failure_all_retries(self):
        # If the fix never passes verification, we end up returning False
        from src.domain.models import TestError, ErrorDetails
        from pathlib import Path
        error_id = uuid4()
        test_error = TestError(
            id=error_id,
            test_file=Path(&quot;tests/test_example.py&quot;),
            test_function=&quot;test_something&quot;,
            error_details=ErrorDetails(error_type=&quot;AssertionError&quot;, message=&quot;Expected X but got Y&quot;)
        )

        mock_repo = MagicMock()
        mock_repo.get_by_id.return_value = test_error

        # AI returns changes each time, but test never passes:
        from src.domain.models import CodeChanges
        mock_ai = MagicMock()
        mock_ai.generate_fix.return_value = CodeChanges(original=&quot;bug&quot;, modified=&quot;fix&quot;)

        mock_test_runner = MagicMock()
        mock_test_runner.run_test_and_check.return_value = False  # never passes

        mock_vcs = MagicMock()
        mock_applier = MagicMock()

        from src.application.usecases import TestFixingService
        service = TestFixingService(
            error_repo=mock_repo,
            ai_manager=mock_ai,
            test_runner=mock_test_runner,
            vcs_manager=mock_vcs,
            change_applier=mock_applier,
            initial_temp=0.4,
            temp_increment=0.1,
            max_retries=2
        )

        success = service.attempt_fix(error_id)
        self.assertFalse(success)
        self.assertEqual(test_error.status, &quot;unfixed&quot;)
        # Verifications:
        # AI generate fix should be called twice (max_retries=2)
        self.assertEqual(mock_ai.generate_fix.call_count, 2)
        # Test runner also called twice
        self.assertEqual(mock_test_runner.run_test_and_check.call_count, 2)
        # VCS commit never called
        mock_vcs.commit_changes.assert_not_called()
        # After each failure, revert should be called
        self.assertEqual(mock_applier.revert.call_count, 2)
</code></pre>
<hr />
<h2 id="step-4-integration-test-optional-at-this-stage">Step 4: Integration Test (Optional at this Stage)</h2>
<p>We could write a high-level test simulating the whole pipeline once we have some code. However, for now, these unit tests are sufficient to guide our initial implementation.</p>
<hr />
<h2 id="summary-of-next-steps">Summary of Next Steps</h2>
<ol>
<li><strong>Run These Tests Now</strong>: They will fail because none of the referenced classes or logic exists.</li>
<li><strong>Implement Minimal Code in <code>src/</code></strong>: Develop just enough code to make these tests pass, step by step.</li>
<li><strong>Refactor the Code Once Tests Are Passing</strong>: Improve the code quality while ensuring tests remain green.</li>
</ol>
<p>We have a clear contract defined by tests, ensuring we only build what’s required and verifying functionality as we proceed.</p>
<p>This test suite and approach should serve as a strong starting point for a TDD-driven rewrite of the <code>pytest-fixer</code> tool’s core functionality.</p>
<hr />
<h2 id="conclusion">Conclusion</h2>
<p>Adopting a Test-Driven Development approach ensures that our development process is guided by well-defined tests, promoting high-quality, maintainable, and reliable code. By following this blueprint, the <code>pytest-fixer</code> project will be built incrementally with a strong foundation, allowing for scalable and efficient development.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/" class="btn btn-neutral float-left" title="DDD for Pytest Fixer Concepts and Implementation Guide"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Repository-Structure-and-Architecture-Design-Proposal/" class="btn btn-neutral float-right" title="Repository Structure and Architecture Design Proposal">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../DDD-for-pytest-fixer-Concepts-and-Implementation-Guide/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Repository-Structure-and-Architecture-Design-Proposal/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
